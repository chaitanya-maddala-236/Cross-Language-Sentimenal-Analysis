{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chaitanya-maddala-236/Cross-Language-Sentimenal-Analysis/blob/main/Cross_Language.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKoVWYQGuUp9"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install transformers datasets scikit-learn pandas numpy matplotlib seaborn torch -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Libraries"
      ],
      "metadata": {
        "id": "kwY7l3w-O1O3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHL-psoQuWKn"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "from datasets import load_dataset\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "from scipy import stats\n",
        "pip install -U transformers accelerate datasets\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7I0UAzvvnsi"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed=42):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# Check device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rki1fXq5vx1V"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(\"=\"*80)\n",
        "print(\"STEP 1: LOADING DATASETS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# 1.1 Load English Training Data (IMDb)\n",
        "print(\"\\nЁЯУе Loading English IMDb dataset...\")\n",
        "try:\n",
        "    imdb_dataset = load_dataset(\"imdb\")\n",
        "\n",
        "    # Sample for faster training (remove sampling for full dataset)\n",
        "    train_data = imdb_dataset['train'].shuffle(seed=42).select(range(5000))\n",
        "    val_data = imdb_dataset['test'].shuffle(seed=42).select(range(1000))\n",
        "\n",
        "    print(f\"тЬЕ English Training samples: {len(train_data)}\")\n",
        "    print(f\"тЬЕ English Validation samples: {len(val_data)}\")\n",
        "    print(f\"Sample: {train_data[0]['text'][:100]}...\")\n",
        "    print(f\"Label: {train_data[0]['label']} (0=Negative, 1=Positive)\")\n",
        "except Exception as e:\n",
        "    print(f\"тЭМ Error loading IMDb: {e}\")\n",
        "    print(\"Note: Run this on Kaggle with internet enabled\")\n",
        "\n",
        "# 1.2 Load Indic Language Test Data\n",
        "print(\"\\nЁЯУе Loading Indic language datasets...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3L7vtMPbv6sk"
      },
      "outputs": [],
      "source": [
        "def create_sample_indic_data():\n",
        "    \"\"\"Create comprehensive sample Indic data for demonstration\"\"\"\n",
        "\n",
        "    # HINDI SAMPLES (100 examples)\n",
        "    hindi_positive = [\n",
        "        'рдпрд╣ рдлрд┐рд▓реНрдо рдмрд╣реБрдд рдЕрдЪреНрдЫреА рд╣реИ',  # This movie is very good\n",
        "        'рд╢рд╛рдирджрд╛рд░ рдЕрднрд┐рдирдп рдФрд░ рдХрд╣рд╛рдиреА',  # Great acting and story\n",
        "        'рдмрд╣реБрдд рд░реЛрдорд╛рдВрдЪрдХ рдФрд░ рдордиреЛрд░рдВрдЬрдХ',  # Very exciting and entertaining\n",
        "        'рдореБрдЭреЗ рдпрд╣ рдлрд┐рд▓реНрдо рдмрд╣реБрдд рдкрд╕рдВрдж рдЖрдИ',  # I really liked this movie\n",
        "        'рдХрдорд╛рд▓ рдХреА рдлрд┐рд▓реНрдо рд╣реИ',  # Amazing movie\n",
        "        'рдмреЗрд╣рддрд░реАрди рдирд┐рд░реНрджреЗрд╢рди рдФрд░ рд╕рдВрдЧреАрдд',  # Excellent direction and music\n",
        "        'рдкрд░рд┐рд╡рд╛рд░ рдХреЗ рд╕рд╛рде рджреЗрдЦрдиреЗ рд▓рд╛рдпрдХ',  # Worth watching with family\n",
        "        'рд╕рднреА рдЕрднрд┐рдиреЗрддрд╛рдУрдВ рдиреЗ рд╢рд╛рдирджрд╛рд░ рдХрд╛рдо рдХрд┐рдпрд╛',  # All actors did great work\n",
        "        'рдпрд╣ рд╕рд╛рд▓ рдХреА рд╕рдмрд╕реЗ рдЕрдЪреНрдЫреА рдлрд┐рд▓реНрдо рд╣реИ',  # This is the best movie of the year\n",
        "        'рдордиреЛрд░рдВрдЬрдХ рдФрд░ рдкреНрд░реЗрд░рдгрд╛рджрд╛рдпрдХ',  # Entertaining and inspiring\n",
        "        'рджрд┐рд▓ рдХреЛ рдЫреВ рдЬрд╛рдиреЗ рд╡рд╛рд▓реА рдХрд╣рд╛рдиреА',  # Heart-touching story\n",
        "        'рдмрдЪреНрдЪреЛрдВ рдХреЗ рд▓рд┐рдП рдПрдХрджрдо рд╕рд╣реА',  # Perfect for children\n",
        "        'рд╣рд░ рдХрд┐рд╕реА рдХреЛ рджреЗрдЦрдиреА рдЪрд╛рд╣рд┐рдП',  # Everyone should watch\n",
        "        'рдкреИрд╕рд╛ рд╡рд╕реВрд▓ рдлрд┐рд▓реНрдо',  # Value for money movie\n",
        "        'рдзрдорд╛рдХреЗрджрд╛рд░ рдПрдХреНрд╢рди рд╕реАрди',  # Explosive action scenes\n",
        "        'рдЧрд╛рдиреЗ рдмрд╣реБрдд рд╕реБрдВрджрд░ рд╣реИрдВ',  # Songs are very beautiful\n",
        "        'рдХрд╣рд╛рдиреА рдореЗрдВ рдирдпрд╛ рдЯреНрд╡рд┐рд╕реНрдЯ',  # New twist in the story\n",
        "        'рд╡рд┐рдЬреБрдЕрд▓ рдЗрдлреЗрдХреНрдЯреНрд╕ рдХрдорд╛рд▓ рдХреЗ',  # Amazing visual effects\n",
        "        'рднрд╛рд╡рдирд╛рддреНрдордХ рдФрд░ рдорд╛рд░реНрдорд┐рдХ',  # Emotional and poignant\n",
        "        'рд╣рд╛рд╕реНрдп рд╕реЗ рднрд░рдкреВрд░',  # Full of humor\n",
        "        'рд░реЛрдорд╛рдВрд╕ рдмрд╣реБрдд рдЕрдЪреНрдЫрд╛ рд╣реИ',  # Romance is very good\n",
        "        'рд╕рдВрджреЗрд╢ рджреЗрдиреЗ рд╡рд╛рд▓реА рдлрд┐рд▓реНрдо',  # Message-oriented movie\n",
        "        'рдпрд╛рджрдЧрд╛рд░ рдлрд┐рд▓реНрдо рдмрди рдЧрдИ',  # Became a memorable movie\n",
        "        'рд╕рд┐рдиреЗрдорд╛рдШрд░реЛрдВ рдореЗрдВ рдЬрд░реВрд░ рджреЗрдЦреЗрдВ',  # Must watch in theaters\n",
        "        'рдкреВрд░реЗ рдкрд░рд┐рд╡рд╛рд░ рдХрд╛ рдордиреЛрд░рдВрдЬрди',  # Entertainment for whole family\n",
        "        'рдмреЗрд╣рддрд░реАрди рдкрдЯрдХрдерд╛',  # Excellent screenplay\n",
        "        'рдЕрднрд┐рдирдп рдХреА рдмреЗрд╣рддрд░реАрди рдорд┐рд╕рд╛рд▓',  # Great example of acting\n",
        "        'рд╕рд┐рдиреЗрдореЗрдЯреЛрдЧреНрд░рд╛рдлреА рд╢рд╛рдирджрд╛рд░ рд╣реИ',  # Cinematography is wonderful\n",
        "        'рд╕рдВрд╡рд╛рдж рдмрд╣реБрдд рдкреНрд░рднрд╛рд╡реА рд╣реИрдВ',  # Dialogues are very effective\n",
        "        'рдХреНрд▓рд╛рдЗрдореЗрдХреНрд╕ рдзрдорд╛рдХреЗрджрд╛рд░',  # Climax is explosive\n",
        "        'рд╕рд╕реНрдкреЗрдВрд╕ рдмрдирд╛ рд░рд╣рддрд╛ рд╣реИ',  # Suspense is maintained\n",
        "        'рд░реЛрдиреЗ рдкрд░ рдордЬрдмреВрд░ рдХрд░ рджреЗрдЧреА',  # Will make you cry\n",
        "        'рд╣рдВрд╕рд╛рдиреЗ рдореЗрдВ рдХрд╛рдордпрд╛рдм',  # Successful in making laugh\n",
        "        'рдкреНрд░реЗрдо рдХрд╣рд╛рдиреА рджрд┐рд▓ рдХреЛ рдЫреВ рдЧрдИ',  # Love story touched the heart\n",
        "        'рджреЗрдЦрддреЗ рд╣реА рд░рд╣реЛрдЧреЗ',  # You'll keep watching\n",
        "        'рд╕рдордп рдмреАрддрдиреЗ рдХрд╛ рдкрддрд╛ рдирд╣реАрдВ рдЪрд▓рд╛',  # Didn't realize time passing\n",
        "        'рд╕рднреА рдЙрдореНрд░ рдХреЗ рд▓реЛрдЧреЛрдВ рдХреЗ рд▓рд┐рдП',  # For all age groups\n",
        "        'рдмреЙрд▓реАрд╡реБрдб рдХреА рд╢рд╛рди',  # Pride of Bollywood\n",
        "        'рдмреНрд▓реЙрдХрдмрд╕реНрдЯрд░ рдлрд┐рд▓реНрдо',  # Blockbuster movie\n",
        "        'рд╣рд┐рдЯ рд╣реЛрдиреЗ рд╡рд╛рд▓реА рдлрд┐рд▓реНрдо',  # Movie that will be a hit\n",
        "        'рджреЛрдмрд╛рд░рд╛ рджреЗрдЦрдиреЗ рд▓рд╛рдпрдХ',  # Worth watching again\n",
        "        'рдкреВрд░реА рддрд░рд╣ рдордиреЛрд░рдВрдЬрдХ',  # Completely entertaining\n",
        "        'рдЧрдЬрдм рдХреА рдлрд┐рд▓реНрдо рд╣реИ',  # Awesome movie\n",
        "        'рд▓рд╛рдЬрд╡рд╛рдм рдкреНрд░рд╕реНрддреБрддрд┐',  # Marvelous presentation\n",
        "        'рджрдорджрд╛рд░ рдЕрднрд┐рдирдп',  # Powerful acting\n",
        "        'рд░реЛрдорд╛рдВрдЪ рд╕реЗ рднрд░рд╛ рдЕрдиреБрднрд╡',  # Thrilling experience\n",
        "        'рдпрдерд╛рд░реНрдерд╡рд╛рджреА рдЪрд┐рддреНрд░рдг',  # Realistic portrayal\n",
        "        'рд╕рд╛рдорд╛рдЬрд┐рдХ рд╕рдВрджреЗрд╢ рдХреЗ рд╕рд╛рде',  # With social message\n",
        "        'рдорд╛рд╕реНрдЯрд░рдкреАрд╕ рдлрд┐рд▓реНрдо',  # Masterpiece movie\n",
        "        'рд╣рд░ рджреГрд╢реНрдп рд╢рд╛рдирджрд╛рд░',  # Every scene is wonderful\n",
        "    ]\n",
        "\n",
        "    hindi_negative = [\n",
        "        'рдореБрдЭреЗ рдпрд╣ рдлрд┐рд▓реНрдо рдкрд╕рдВрдж рдирд╣реАрдВ рдЖрдИ',  # I didn't like this movie\n",
        "        'рдмреЗрдХрд╛рд░ рдлрд┐рд▓реНрдо рд╕рдордп рдХреА рдмрд░реНрдмрд╛рджреА',  # Bad movie waste of time\n",
        "        'рдХрд╣рд╛рдиреА рдмрд╣реБрдд рдХрдордЬреЛрд░ рд╣реИ',  # Story is very weak\n",
        "        'рдЙрдмрд╛рдК рдФрд░ рд▓рдВрдмреА рдлрд┐рд▓реНрдо',  # Boring and long movie\n",
        "        'рдкреИрд╕реЗ рдХрд╛ рдмрд░реНрдмрд╛рдж',  # Waste of money\n",
        "        'рдЕрднрд┐рдирдп рдмрд┐рд▓реНрдХреБрд▓ рдирд╣реАрдВ рдЬрдорд╛',  # Acting didn't work at all\n",
        "        'рдирд┐рд░реНрджреЗрд╢рди рдореЗрдВ рдХрдореА',  # Lack in direction\n",
        "        'рд╕рдВрдЧреАрдд рднреА рдЕрдЪреНрдЫрд╛ рдирд╣реАрдВ',  # Music is also not good\n",
        "        'рджреЗрдЦрдиреЗ рд▓рд╛рдпрдХ рдирд╣реАрдВ',  # Not worth watching\n",
        "        'рд╕рдордп рдЦрд░рд╛рдм рд╣реЛ рдЧрдпрд╛',  # Time got wasted\n",
        "        'рдХреЛрдИ рдирдпрд╛рдкрди рдирд╣реАрдВ',  # Nothing new\n",
        "        'рдкреБрд░рд╛рдиреА рдХрд╣рд╛рдиреА рдлрд┐рд░ рд╕реЗ',  # Old story again\n",
        "        'рдмрдЪреНрдЪреЛрдВ рдХреЛ рдордд рджрд┐рдЦрд╛рдирд╛',  # Don't show to children\n",
        "        'рдмрд╣реБрдд рдирд┐рд░рд╛рд╢рд╛рдЬрдирдХ',  # Very disappointing\n",
        "        'рдПрдХреНрд╢рди рднреА рдмреЗрдХрд╛рд░',  # Action also useless\n",
        "        'рдЧрд╛рдиреЗ рдХрд╛рдиреЛрдВ рдХреЛ рддрдХрд▓реАрдл',  # Songs hurt the ears\n",
        "        'рдЯреНрд╡рд┐рд╕реНрдЯ рдХрд╛ рдЕрднрд╛рд╡',  # Lack of twist\n",
        "        'рд╡рд┐рдЬреБрдЕрд▓ рдЗрдлреЗрдХреНрдЯреНрд╕ рдШрдЯрд┐рдпрд╛',  # Poor visual effects\n",
        "        'рднрд╛рд╡рдирд╛рддреНрдордХ рд░реВрдк рд╕реЗ рдХрдордЬреЛрд░',  # Emotionally weak\n",
        "        'рд╣рд╛рд╕реНрдп рдлреАрдХрд╛ рд╣реИ',  # Comedy is bland\n",
        "        'рд░реЛрдорд╛рдВрд╕ рдЬрдмрд░рджрд╕реНрддреА рдХрд╛',  # Forced romance\n",
        "        'рдХреЛрдИ рд╕рдВрджреЗрд╢ рдирд╣реАрдВ',  # No message\n",
        "        'рднреВрд▓рдиреЗ рдпреЛрдЧреНрдп рдлрд┐рд▓реНрдо',  # Forgettable movie\n",
        "        'рдШрд░ рдкрд░ рднреА рдордд рджреЗрдЦреЗрдВ',  # Don't watch even at home\n",
        "        'рдкрд░рд┐рд╡рд╛рд░ рдХреЗ рд╕рд╛рде рд╢рд░реНрдорд┐рдВрджрдЧреА',  # Embarrassment with family\n",
        "        'рдкрдЯрдХрдерд╛ рдореЗрдВ рдЫреЗрдж',  # Holes in screenplay\n",
        "        'рдЕрднрд┐рдирдп рдмрдирд╛рд╡рдЯреА рд▓рдЧрд╛',  # Acting seemed artificial\n",
        "        'рд╕рд┐рдиреЗрдореЗрдЯреЛрдЧреНрд░рд╛рдлреА рдФрд╕рдд',  # Average cinematography\n",
        "        'рд╕рдВрд╡рд╛рдж рдмрдЪрдХрд╛рдиреЗ рд╣реИрдВ',  # Dialogues are childish\n",
        "        'рдХреНрд▓рд╛рдЗрдореЗрдХреНрд╕ рдирд┐рд░рд╛рд╢ рдХрд░рддрд╛ рд╣реИ',  # Climax disappoints\n",
        "        'рд╕рд╕реНрдкреЗрдВрд╕ рдмрд┐рд▓реНрдХреБрд▓ рдирд╣реАрдВ',  # No suspense at all\n",
        "        'рд░реБрд▓рд╛рдиреЗ рдореЗрдВ рдирд╛рдХрд╛рдо',  # Failed to make cry\n",
        "        'рд╣рдВрд╕рд╛рдиреЗ рдХреА рдХреЛрд╢рд┐рд╢ рдлреЗрд▓',  # Failed attempt to make laugh\n",
        "        'рдкреНрд░реЗрдо рдХрд╣рд╛рдиреА рдШрд┐рд╕реА рдкрд┐рдЯреА',  # Love story is cliched\n",
        "        'рджреЗрдЦрддреЗ рд╣реБрдП рдиреАрдВрдж рдЖрдИ',  # Felt sleepy while watching\n",
        "        'рд╕рдордп рдмрд░реНрдмрд╛рдж рд╣реЛ рдЧрдпрд╛',  # Time got wasted\n",
        "        'рдмрдЪреНрдЪреЛрдВ рдХреЛ рдмреЛрд░ рдХрд░реЗрдЧреА',  # Will bore children\n",
        "        'рдмреЙрд▓реАрд╡реБрдб рдХреА рд╕рдмрд╕реЗ рдЦрд░рд╛рдм',  # Worst of Bollywood\n",
        "        'рдлреНрд▓реЙрдк рд╣реЛрдиреЗ рд╡рд╛рд▓реА рдлрд┐рд▓реНрдо',  # Movie that will flop\n",
        "        'рдПрдХ рдмрд╛рд░ рднреА рдордд рджреЗрдЦреЛ',  # Don't watch even once\n",
        "        'рдкреВрд░реА рддрд░рд╣ рдмреЗрдХрд╛рд░',  # Completely useless\n",
        "        'рдШрдЯрд┐рдпрд╛ рдлрд┐рд▓реНрдо рд╣реИ',  # Poor quality movie\n",
        "        'рдмреБрд░реА рдкреНрд░рд╕реНрддреБрддрд┐',  # Bad presentation\n",
        "        'рдХрдордЬреЛрд░ рдЕрднрд┐рдирдп',  # Weak acting\n",
        "        'рд░реЛрдорд╛рдВрдЪ рдХрд╛ рдЕрднрд╛рд╡',  # Lack of thrill\n",
        "        'рдЕрд╡рд╛рд╕реНрддрд╡рд┐рдХ рдЪрд┐рддреНрд░рдг',  # Unrealistic portrayal\n",
        "        'рд╡реНрдпрд░реНрде рдХрд╛ рд╕рд╛рдорд╛рдЬрд┐рдХ рд╕рдВрджреЗрд╢',  # Useless social message\n",
        "        'рдЕрд╕рдлрд▓ рдлрд┐рд▓реНрдо',  # Failed movie\n",
        "        'рд╣рд░ рджреГрд╢реНрдп рдЙрдмрд╛рдК',  # Every scene is boring\n",
        "    ]\n",
        "\n",
        "    hindi_data = pd.DataFrame({\n",
        "        'text': hindi_positive + hindi_negative,\n",
        "        'label': [1] * len(hindi_positive) + [0] * len(hindi_negative)\n",
        "    })\n",
        "\n",
        "    # TELUGU SAMPLES (100 examples)\n",
        "    telugu_positive = [\n",
        "        'р░И р░╕р░┐р░ир░┐р░ор░╛ р░Ър░╛р░▓р░╛ р░мр░╛р░Чр▒Бр░Вр░жр░┐',  # This movie is very good\n",
        "        'р░Ер░жр▒Нр░нр▒Бр░др░ор▒Ир░и р░ир░Яр░и р░ор░░р░┐р░пр▒Б р░Хр░е',  # Wonderful acting and story\n",
        "        'р░Ър░╛р░▓р░╛ р░Йр░др▒Нр░др▒Зр░Ьр░Хр░░р░ор▒Ир░и',  # Very exciting\n",
        "        'р░ир░╛р░Хр▒Б р░И р░╕р░┐р░ир░┐р░ор░╛ р░Ър░╛р░▓р░╛ р░ир░Ър▒Нр░Ър░┐р░Вр░жр░┐',  # I really liked this movie\n",
        "        'р░Ер░жр▒Нр░нр▒Бр░др░ор▒Ир░и р░╕р░┐р░ир░┐р░ор░╛',  # Amazing movie\n",
        "        'р░Чр▒Кр░кр▒Нр░к р░жр░░р▒Нр░╢р░Хр░др▒Нр░╡р░В р░ор░░р░┐р░пр▒Б р░╕р░Вр░Чр▒Ар░др░В',  # Great direction and music\n",
        "        'р░Хр▒Бр░Яр▒Бр░Вр░мр░Вр░др▒Л р░Ър▒Вр░бр░жр░Чр░┐р░ир░жр░┐',  # Worth watching with family\n",
        "        'р░Ер░Вр░жр░░р▒Б р░ир░Яр▒Ар░ир░Яр▒Бр░▓р▒Б р░Ер░жр▒Нр░нр▒Бр░др░Вр░Чр░╛ р░ир░Яр░┐р░Вр░Ър░╛р░░р▒Б',  # All actors acted wonderfully\n",
        "        'р░И р░╕р░Вр░╡р░др▒Нр░╕р░░р░кр▒Б р░Йр░др▒Нр░др░о р░Ър░┐р░др▒Нр░░р░В',  # Best movie of the year\n",
        "        'р░╡р░┐р░ир▒Лр░жр░нр░░р░┐р░др░В р░ор░░р░┐р░пр▒Б р░╕р▒Нр░лр▒Вр░░р▒Нр░др░┐р░жр░╛р░пр░Хр░В',  # Entertaining and inspiring\n",
        "        'р░╣р▒Гр░жр░пр░╛р░ир▒Нр░ир░┐ р░др░╛р░Хр▒З р░Хр░е',  # Heart-touching story\n",
        "        'р░кр░┐р░▓р▒Нр░▓р░▓р░Хр▒Б р░╕р░░р░┐р░кр▒Лр░пр▒Зр░жр░┐',  # Perfect for children\n",
        "        'р░Ер░Вр░жр░░р▒В р░др░кр▒Нр░кр░Х р░Ър▒Вр░бр░╛р░▓р░┐',  # Everyone must watch\n",
        "        'р░бр░мр▒Нр░мр▒Б р░╡р░┐р░▓р▒Бр░╡р▒Ир░ир░жр░┐',  # Value for money\n",
        "        'р░Ер░жр▒Нр░нр▒Бр░др░ор▒Ир░и р░пр░╛р░Хр▒Нр░╖р░ир▒Н р░╕р▒Ар░ир▒Нр░▓р▒Б',  # Amazing action scenes\n",
        "        'р░кр░╛р░Яр░▓р▒Б р░Ър░╛р░▓р░╛ р░Ер░Вр░жр░Вр░Чр░╛ р░Йр░ир▒Нр░ир░╛р░пр░┐',  # Songs are very beautiful\n",
        "        'р░Хр░ер░▓р▒Л р░Хр▒Кр░др▒Нр░д р░Яр▒Нр░╡р░┐р░╕р▒Нр░Яр▒Н',  # New twist in story\n",
        "        'р░╡р░┐р░Ьр▒Бр░╡р░▓р▒Н р░Ор░лр▒Жр░Хр▒Нр░Яр▒Нр░╕р▒Н р░Ер░жр▒Нр░нр▒Бр░др░ор▒Ир░ир░╡р░┐',  # Visual effects are amazing\n",
        "        'р░нр░╛р░╡р▒Лр░жр▒Нр░╡р▒Зр░Чр░нр░░р░┐р░др░ор▒Ир░ир░жр░┐',  # Emotional\n",
        "        'р░╣р░╛р░╕р▒Нр░пр░Вр░др▒Л р░ир░┐р░Вр░бр░┐р░ир░жр░┐',  # Full of humor\n",
        "        'р░кр▒Нр░░р▒Зр░ор░Хр░е р░Ър░╛р░▓р░╛ р░мр░╛р░Чр▒Бр░Вр░жр░┐',  # Love story is very good\n",
        "        'р░╕р░Вр░жр▒Зр░╢р░╛р░ир▒Нр░ир░┐ р░Зр░Ър▒Нр░Ър▒З р░╕р░┐р░ир░┐р░ор░╛',  # Message-giving movie\n",
        "        'р░ор░░р░кр▒Бр░░р░╛р░ир░┐ р░╕р░┐р░ир░┐р░ор░╛',  # Unforgettable movie\n",
        "        'р░ер░┐р░пр▒Зр░Яр░░р▒Нр░▓р░▓р▒Л р░др░кр▒Нр░кр░Х р░Ър▒Вр░бр░╛р░▓р░┐',  # Must watch in theaters\n",
        "        'р░ор▒Кр░др▒Нр░др░В р░Хр▒Бр░Яр▒Бр░Вр░мр░╛р░ир░┐р░Хр░┐ р░╡р░┐р░ир▒Лр░жр░В',  # Entertainment for whole family\n",
        "        'р░Ер░жр▒Нр░нр▒Бр░др░ор▒Ир░и р░╕р▒Нр░Хр▒Нр░░р▒Ар░ир▒НтАМр░кр▒Нр░▓р▒З',  # Excellent screenplay\n",
        "        'р░ир░Яр░и р░пр▒Кр░Хр▒Нр░Х р░Чр▒Кр░кр▒Нр░к р░Йр░жр░╛р░╣р░░р░г',  # Great example of acting\n",
        "        'р░╕р░┐р░ир░┐р░ор░╛р░Яр▒Лр░Чр▒Нр░░р░лр▒А р░Ер░жр▒Нр░нр▒Бр░др░ор▒Ир░ир░жр░┐',  # Cinematography is wonderful\n",
        "        'р░бр▒Ир░▓р░╛р░Чр▒Нр░╕р▒Н р░Ър░╛р░▓р░╛ р░кр▒Нр░░р░нр░╛р░╡р░╡р░Вр░др░ор▒Ир░ир░╡р░┐',  # Dialogues are very effective\n",
        "        'р░Хр▒Нр░▓р▒Ир░ор░╛р░Хр▒Нр░╕р▒Н р░Ер░жр▒Нр░нр▒Бр░др░ор▒Ир░ир░жр░┐',  # Climax is amazing\n",
        "        'р░╕р░╕р▒Нр░кр▒Жр░ир▒Нр░╕р▒Н р░Хр▒Кр░ир░╕р░╛р░Чр▒Бр░др▒Бр░Вр░жр░┐',  # Suspense continues\n",
        "        'р░Пр░бр▒Нр░кр░┐р░Вр░Ър▒Зр░▓р░╛ р░Йр░Вр░жр░┐',  # Makes you cry\n",
        "        'р░ир░╡р▒Нр░╡р░┐р░Вр░Ър░бр░Вр░▓р▒Л р░╡р░┐р░Ьр░пр░╡р░Вр░др░В',  # Successful in making laugh\n",
        "        'р░кр▒Нр░░р▒Зр░ор░Хр░е р░╣р▒Гр░жр░пр░╛р░ир▒Нр░ир░┐ р░др░╛р░Хр░┐р░Вр░жр░┐',  # Love story touched heart\n",
        "        'р░Ър▒Вр░╕р▒Нр░др▒Вр░ир▒З р░Йр░Вр░Яр░╛р░░р▒Б',  # You'll keep watching\n",
        "        'р░╕р░ор░пр░В р░Чр░бр░┐р░Ър░┐р░Вр░жр░ир░┐ р░др▒Жр░▓р░┐р░пр░▓р▒Зр░жр▒Б',  # Didn't realize time passing\n",
        "        'р░Ер░ир▒Нр░ир░┐ р░╡р░пр░╕р▒Бр░▓ р░╡р░╛р░░р░┐р░Хр░┐',  # For all ages\n",
        "        'р░др▒Жр░▓р▒Бр░Чр▒Б р░╕р░┐р░ир░┐р░ор░╛ р░Чр░░р▒Нр░╡р░В',  # Pride of Telugu cinema\n",
        "        'р░мр▒Нр░▓р░╛р░Хр▒НтАМр░мр░╕р▒Нр░Яр░░р▒Н р░╕р░┐р░ир░┐р░ор░╛',  # Blockbuster movie\n",
        "        'р░╣р░┐р░Яр▒Н р░Ер░пр▒Нр░пр▒З р░╕р░┐р░ир░┐р░ор░╛',  # Movie that will be hit\n",
        "        'р░ор░│р▒Нр░│р▒А р░Ър▒Вр░бр░жр░Чр░┐р░ир░жр░┐',  # Worth watching again\n",
        "        'р░кр▒Вр░░р▒Нр░др░┐р░Чр░╛ р░╡р░┐р░ир▒Лр░жр░нр░░р░┐р░др░В',  # Completely entertaining\n",
        "        'р░Ер░жр▒Нр░нр▒Бр░др░ор▒Ир░и р░╕р░┐р░ир░┐р░ор░╛',  # Awesome movie\n",
        "        'р░Ер░жр▒Нр░нр▒Бр░др░ор▒Ир░и р░кр▒Нр░░р░жр░░р▒Нр░╢р░и',  # Marvelous presentation\n",
        "        'р░╢р░Хр▒Нр░др░┐р░╡р░Вр░др░ор▒Ир░и р░ир░Яр░и',  # Powerful acting\n",
        "        'р░ер▒Нр░░р░┐р░▓р▒Нр░▓р░┐р░Вр░Чр▒Н р░Ер░ир▒Бр░нр░╡р░В',  # Thrilling experience\n",
        "        'р░╡р░╛р░╕р▒Нр░др░╡р░┐р░Х р░Ър░┐р░др▒Нр░░р░г',  # Realistic portrayal\n",
        "        'р░╕р░╛р░ор░╛р░Ьр░┐р░Х р░╕р░Вр░жр▒Зр░╢р░Вр░др▒Л',  # With social message\n",
        "        'р░ор░╛р░╕р▒Нр░Яр░░р▒НтАМр░кр▒Ар░╕р▒Н р░╕р░┐р░ир░┐р░ор░╛',  # Masterpiece movie\n",
        "        'р░кр▒Нр░░р░др░┐ р░╕р░ир▒Нр░ир░┐р░╡р▒Зр░╢р░В р░Ер░жр▒Нр░нр▒Бр░др░В',  # Every scene is wonderful\n",
        "    ]\n",
        "\n",
        "    telugu_negative = [\n",
        "        'р░ир░╛р░Хр▒Б р░И р░╕р░┐р░ир░┐р░ор░╛ р░ир░Ър▒Нр░Ър░▓р▒Зр░жр▒Б',  # I didn't like this movie\n",
        "        'р░╡р▒Нр░пр░░р▒Нр░е р░╕р░┐р░ир░┐р░ор░╛',  # Waste movie\n",
        "        'р░Хр░е р░Ър░╛р░▓р░╛ р░мр░▓р░╣р▒Ар░ир░Вр░Чр░╛ р░Йр░Вр░жр░┐',  # Story is very weak\n",
        "        'р░мр▒Лр░░р░┐р░Вр░Чр▒Н р░ор░░р░┐р░пр▒Б р░кр▒Кр░бр░╡р▒Ир░и р░╕р░┐р░ир░┐р░ор░╛',  # Boring and long movie\n",
        "        'р░бр░мр▒Нр░мр▒Б р░╡р▒Гр░ер░╛',  # Waste of money\n",
        "        'р░ир░Яр░и р░мр░┐р░▓р▒Нр░Хр▒Бр░▓р▒Н р░Ьр░ор░▓р▒Зр░жр▒Б',  # Acting didn't work at all\n",
        "        'р░жр░░р▒Нр░╢р░Хр░др▒Нр░╡р░Вр░▓р▒Л р░▓р▒Лр░кр░В',  # Lack in direction\n",
        "        'р░╕р░Вр░Чр▒Ар░др░В р░Хр▒Вр░бр░╛ р░мр░╛р░Чр▒Лр░▓р▒Зр░жр▒Б',  # Music is also not good\n",
        "        'р░Ър▒Вр░бр░жр░Чр░┐р░ир░жр░┐ р░Хр░╛р░жр▒Б',  # Not worth watching\n",
        "        'р░╕р░ор░пр░В р░╡р▒Гр░ер░╛ р░Ер░пр░┐р░Вр░жр░┐',  # Time got wasted\n",
        "        'р░Пр░ор▒А р░Хр▒Кр░др▒Нр░др░жр░┐ р░▓р▒Зр░жр▒Б',  # Nothing new\n",
        "        'р░кр░╛р░д р░Хр░е р░ор░│р▒Нр░│р▒А',  # Old story again\n",
        "        'р░кр░┐р░▓р▒Нр░▓р░▓р░Хр▒Б р░Ър▒Вр░кр░┐р░Вр░Ър░╡р░жр▒Нр░жр▒Б',  # Don't show to children\n",
        "        'р░Ър░╛р░▓р░╛ р░ир░┐р░░р░╛р░╢р░кр░░р░┐р░Ър░┐р░Вр░жр░┐',  # Very disappointing\n",
        "        'р░пр░╛р░Хр▒Нр░╖р░ир▒Н р░Хр▒Вр░бр░╛ р░╡р▒Нр░пр░░р▒Нр░ер░В',  # Action also useless\n",
        "        'р░кр░╛р░Яр░▓р▒Б р░Ър▒Жр░╡р▒Бр░▓р░Хр▒Б р░мр░╛р░з',  # Songs hurt the ears\n",
        "        'р░Яр▒Нр░╡р░┐р░╕р▒Нр░Яр▒Н р░▓р▒Зр░жр▒Б',  # No twist\n",
        "        'р░╡р░┐р░Ьр▒Бр░╡р░▓р▒Н р░Ор░лр▒Жр░Хр▒Нр░Яр▒Нр░╕р▒Н р░Ър▒Жр░др▒Нр░д',  # Poor visual effects\n",
        "        'р░нр░╛р░╡р▒Лр░жр▒Нр░╡р▒Зр░Чр░Вр░Чр░╛ р░мр░▓р░╣р▒Ар░ир░В',  # Emotionally weak\n",
        "        'р░╣р░╛р░╕р▒Нр░пр░В р░Ър░кр▒Нр░кр░Чр░╛ р░Йр░Вр░жр░┐',  # Comedy is bland\n",
        "        'р░мр░▓р░╡р░Вр░др░кр▒Б р░░р▒Кр░ор░╛р░ир▒Нр░╕р▒Н',  # Forced romance\n",
        "        'р░Ор░▓р░╛р░Вр░Яр░┐ р░╕р░Вр░жр▒Зр░╢р░В р░▓р▒Зр░жр▒Б',  # No message\n",
        "        'р░ор░░р▒Нр░Ър░┐р░кр▒Лр░пр▒З р░╕р░┐р░ир░┐р░ор░╛',  # Forgettable movie\n",
        "        'р░Зр░Вр░Яр▒Нр░▓р▒Л р░Хр▒Вр░бр░╛ р░Ър▒Вр░бр░╡р░жр▒Нр░жр▒Б',  # Don't watch even at home\n",
        "        'р░Хр▒Бр░Яр▒Бр░Вр░мр░Вр░др▒Л р░Зр░мр▒Нр░мр░Вр░жр░┐',  # Embarrassment with family\n",
        "        'р░╕р▒Нр░Хр▒Нр░░р▒Ар░ир▒НтАМр░кр▒Нр░▓р▒Зр░▓р▒Л р░░р░Вр░зр▒Нр░░р░╛р░▓р▒Б',  # Holes in screenplay\n",
        "        'р░ир░Яр░и р░Хр▒Гр░др▒Нр░░р░┐р░ор░Вр░Чр░╛ р░Йр░Вр░жр░┐',  # Acting seemed artificial\n",
        "        'р░╕р░┐р░ир░┐р░ор░╛р░Яр▒Лр░Чр▒Нр░░р░лр▒А р░╕р░Чр░Яр▒Б',  # Average cinematography\n",
        "        'р░бр▒Ир░▓р░╛р░Чр▒Нр░╕р▒Н р░кр░┐р░▓р▒Нр░▓р░др░ир░Вр░Чр░╛ р░Йр░ир▒Нр░ир░╛р░пр░┐',  # Dialogues are childish\n",
        "        'р░Хр▒Нр░▓р▒Ир░ор░╛р░Хр▒Нр░╕р▒Н р░ир░┐р░░р░╛р░╢р░кр░░р▒Бр░╕р▒Нр░др▒Бр░Вр░жр░┐',  # Climax disappoints\n",
        "        'р░╕р░╕р▒Нр░кр▒Жр░ир▒Нр░╕р▒Н р░мр░┐р░▓р▒Нр░Хр▒Бр░▓р▒Н р░▓р▒Зр░жр▒Б',  # No suspense at all\n",
        "        'р░Пр░бр▒Нр░кр░┐р░Вр░Ър░бр░Вр░▓р▒Л р░╡р░┐р░лр░▓р░В',  # Failed to make cry\n",
        "        'р░ир░╡р▒Нр░╡р░┐р░Вр░Ър▒З р░кр▒Нр░░р░пр░др▒Нр░ир░В р░╡р░┐р░лр░▓р░В',  # Failed attempt to make laugh\n",
        "        'р░кр▒Нр░░р▒Зр░ор░Хр░е р░Хр▒Нр░▓р░┐р░╖р▒Нр░Яр░ор▒Ир░ир░жр░┐',  # Love story is cliched\n",
        "        'р░Ър▒Вр░╕р▒Нр░др▒В р░ир░┐р░жр▒Нр░░ р░╡р░Ър▒Нр░Ър░┐р░Вр░жр░┐',  # Felt sleepy while watching\n",
        "        'р░╕р░ор░пр░В р░╡р▒Гр░ер░╛ р░Ер░пр░┐р░Вр░жр░┐',  # Time got wasted\n",
        "        'р░кр░┐р░▓р▒Нр░▓р░▓р░ир▒Б р░мр▒Лр░░р▒Н р░Ър▒Зр░╕р▒Нр░др▒Бр░Вр░жр░┐',  # Will bore children\n",
        "        'р░др▒Жр░▓р▒Бр░Чр▒Б р░╕р░┐р░ир░┐р░ор░╛р░▓р▒Л р░Ър▒Жр░др▒Нр░д',  # Worst of Telugu cinema\n",
        "        'р░лр▒Нр░▓р░╛р░кр▒Н р░Ер░пр▒Нр░пр▒З р░╕р░┐р░ир░┐р░ор░╛',  # Movie that will flop\n",
        "        'р░Тр░Хр▒Нр░Хр░╕р░╛р░░р░┐ р░Хр▒Вр░бр░╛ р░Ър▒Вр░бр░╡р░жр▒Нр░жр▒Б',  # Don't watch even once\n",
        "        'р░кр▒Вр░░р▒Нр░др░┐р░Чр░╛ р░╡р▒Нр░пр░░р▒Нр░ер░В',  # Completely useless\n",
        "        'р░Ър▒Жр░др▒Нр░д р░╕р░┐р░ир░┐р░ор░╛',  # Poor quality movie\n",
        "        'р░Ър▒Жр░бр▒Нр░б р░кр▒Нр░░р░жр░░р▒Нр░╢р░и',  # Bad presentation\n",
        "        'р░мр░▓р░╣р▒Ар░ир░ор▒Ир░и р░ир░Яр░и',  # Weak acting\n",
        "        'р░ер▒Нр░░р░┐р░▓р▒Н р░▓р▒Зр░жр▒Б',  # No thrill\n",
        "        'р░Ер░╡р░╛р░╕р▒Нр░др░╡р░┐р░Х р░Ър░┐р░др▒Нр░░р░г',  # Unrealistic portrayal\n",
        "        'р░╡р▒Нр░пр░░р▒Нр░ер░ор▒Ир░и р░╕р░╛р░ор░╛р░Ьр░┐р░Х р░╕р░Вр░жр▒Зр░╢р░В',  # Useless social message\n",
        "        'р░╡р░┐р░лр░▓р░ор▒Ир░и р░╕р░┐р░ир░┐р░ор░╛',  # Failed movie\n",
        "        'р░кр▒Нр░░р░др░┐ р░╕р░ир▒Нр░ир░┐р░╡р▒Зр░╢р░В р░мр▒Лр░░р░┐р░Вр░Чр▒Н',  # Every scene is boring\n",
        "    ]\n",
        "\n",
        "    telugu_data = pd.DataFrame({\n",
        "        'text': telugu_positive + telugu_negative,\n",
        "        'label': [1] * len(telugu_positive) + [0] * len(telugu_negative)\n",
        "    })\n",
        "\n",
        "    # TAMIL SAMPLES (100 examples)\n",
        "    tamil_positive = [\n",
        "        'роЗроирпНрод родро┐ро░рпИрокрпНрокроЯроорпН рооро┐роХро╡рпБроорпН роиройрпНро▒ро╛роХ роЙро│рпНро│родрпБ',  # This movie is very good\n",
        "        'роЕро░рпБроорпИропро╛рой роироЯро┐рокрпНрокрпБ рооро▒рпНро▒рпБроорпН роХродрпИ',  # Great acting and story\n",
        "        'рооро┐роХро╡рпБроорпН роЪрпБро╡ро╛ро░ро╕рпНропрооро╛рой',  # Very interesting\n",
        "        'роОройроХрпНроХрпБ роЗроирпНрод рокроЯроорпН рооро┐роХро╡рпБроорпН рокро┐роЯро┐родрпНродро┐ро░рпБроирпНродродрпБ',  # I really liked this movie\n",
        "        'роЕро▒рпНрокрпБродрооро╛рой родро┐ро░рпИрокрпНрокроЯроорпН',  # Amazing movie\n",
        "        'роЪро┐ро▒роирпНрод роЗропроХрпНроХроорпН рооро▒рпНро▒рпБроорпН роЗроЪрпИ',  # Great direction and music\n",
        "        'роХрпБроЯрпБроорпНрокродрпНродрпБроЯройрпН рокро╛ро░рпНроХрпНроХ родроХрпБроирпНродродрпБ',  # Worth watching with family\n",
        "        'роЕройрпИродрпНродрпБ роироЯро┐роХро░рпНроХро│рпБроорпН роЕро░рпБроорпИропро╛роХ роироЯро┐родрпНродрпБро│рпНро│ройро░рпН',  # All actors acted wonderfully\n",
        "        'роЗроирпНрод роЖрогрпНроЯро┐ройрпН роЪро┐ро▒роирпНрод рокроЯроорпН',  # Best movie of the year\n",
        "        'рокрпКро┤рпБродрпБрокрпЛроХрпНроХрпБ рооро▒рпНро▒рпБроорпН роКроХрпНроХрооро│ро┐роХрпНроХрпБроорпН',  # Entertaining and inspiring\n",
        "        'роЗродропродрпНродрпИродрпН родрпКроЯрпБроорпН роХродрпИ',  # Heart-touching story\n",
        "        'роХрпБро┤роирпНродрпИроХро│рпБроХрпНроХрпБ роПро▒рпНро▒родрпБ',  # Perfect for children\n",
        "        'роЕройрпИро╡ро░рпБроорпН роХрогрпНроЯро┐рокрпНрокро╛роХ рокро╛ро░рпНроХрпНроХ ро╡рпЗрогрпНроЯрпБроорпН',  # Everyone must watch\n",
        "        'рокрогродрпНродро┐ро▒рпНроХрпБ роородро┐рокрпНрокрпБро│рпНро│родрпБ',  # Value for money\n",
        "        'роЕро▒рпНрокрпБродрооро╛рой роЖроХрпНро╖ройрпН роХро╛роЯрпНроЪро┐роХро│рпН',  # Amazing action scenes\n",
        "        'рокро╛роЯро▓рпНроХро│рпН рооро┐роХро╡рпБроорпН роЕро┤роХро╛роХ роЙро│рпНро│рой',  # Songs are very beautiful\n",
        "        'роХродрпИропро┐ро▓рпН рокрпБродро┐роп родро┐ро░рпБрокрпНрокроорпН',  # New twist in story\n",
        "        'ро╡ро┐ро╖рпБро╡ро▓рпН роОроГрокрпЖроХрпНроЯрпНро╕рпН роЕро▒рпНрокрпБродрооро╛ройро╡рпИ',  # Visual effects are amazing\n",
        "        'роЙрогро░рпНроЪрпНроЪро┐роХро░рооро╛ройродрпБ',  # Emotional\n",
        "        'роироХрпИроЪрпНроЪрпБро╡рпИропро╛ро▓рпН роиро┐ро░рокрпНрокрокрпНрокроЯрпНроЯродрпБ',  # Full of humor\n",
        "        'роХро╛родро▓рпН роХродрпИ рооро┐роХро╡рпБроорпН роиройрпНро▒ро╛роХ роЙро│рпНро│родрпБ',  # Love story is very good\n",
        "        'роЪрпЖропрпНродро┐ роХрпКроЯрпБроХрпНроХрпБроорпН рокроЯроорпН',  # Message-giving movie\n",
        "        'рооро▒роХрпНроХ роорпБроЯро┐ропро╛род рокроЯроорпН',  # Unforgettable movie\n",
        "        'родро┐ро░рпИропро░роЩрпНроХрпБроХро│ро┐ро▓рпН роХрогрпНроЯро┐рокрпНрокро╛роХ рокро╛ро░рпНроХрпНроХ ро╡рпЗрогрпНроЯрпБроорпН',  # Must watch in theaters\n",
        "        'роорпБро┤рпБ роХрпБроЯрпБроорпНрокродрпНродро┐ро▒рпНроХрпБроорпН рокрпКро┤рпБродрпБрокрпЛроХрпНроХрпБ',  # Entertainment for whole family\n",
        "        'роЕро▒рпНрокрпБродрооро╛рой родро┐ро░рпИроХрпНроХродрпИ',  # Excellent screenplay\n",
        "        'роироЯро┐рокрпНрокро┐ройрпН роЪро┐ро▒роирпНрод роЙродро╛ро░рогроорпН',  # Great example of acting\n",
        "        'роТро│ро┐рокрпНрокродро┐ро╡рпБ роЕро▒рпНрокрпБродрооро╛ройродрпБ',  # Cinematography is wonderful\n",
        "        'ро╡роЪройроЩрпНроХро│рпН рооро┐роХро╡рпБроорпН рокропройрпБро│рпНро│ро╡рпИ',  # Dialogues are very effective\n",
        "        'роХрпНро│рпИрооро╛роХрпНро╕рпН роЕро▒рпНрокрпБродрооро╛ройродрпБ',  # Climax is amazing\n",
        "        'роЪро╕рпНрокрпЖройрпНро╕рпН родрпКроЯро░рпНроХро┐ро▒родрпБ',  # Suspense continues\n",
        "        'роЕро┤ ро╡рпИроХрпНроХрпБроорпН',  # Makes you cry\n",
        "        'роЪро┐ро░ро┐роХрпНроХ ро╡рпИрокрпНрокродро┐ро▓рпН ро╡рпЖро▒рпНро▒ро┐',  # Successful in making laugh\n",
        "        'роХро╛родро▓рпН роХродрпИ роЗродропродрпНродрпИродрпН родрпКроЯрпНроЯродрпБ',  # Love story touched heart\n",
        "        'рокро╛ро░рпНродрпНродрпБроХрпН роХрпКрогрпНроЯрпЗ роЗро░рпБрокрпНрокрпАро░рпНроХро│рпН',  # You'll keep watching\n",
        "        'роирпЗро░роорпН роХроЯроирпНродродрпЗ родрпЖро░ро┐ропро╡ро┐ро▓рпНро▓рпИ',  # Didn't realize time passing\n",
        "        'роЕройрпИродрпНродрпБ ро╡ропродро┐ройро░рпБроХрпНроХрпБроорпН',  # For all ages\n",
        "        'родрооро┐ро┤рпН роЪро┐ройро┐рооро╛ро╡ро┐ройрпН рокрпЖро░рпБроорпИ',  # Pride of Tamil cinema\n",
        "        'рокро┐ро│ро╛роХрпНрокро╕рпНроЯро░рпН рокроЯроорпН',  # Blockbuster movie\n",
        "        'ро╣ро┐роЯрпН роЖроХрпБроорпН рокроЯроорпН',  # Movie that will be hit\n",
        "        'роорпАрогрпНроЯрпБроорпН рокро╛ро░рпНроХрпНроХ родроХрпБроирпНродродрпБ',  # Worth watching again\n",
        "        'роорпБро▒рпНро▒ро┐ро▓рпБроорпН рокрпКро┤рпБродрпБрокрпЛроХрпНроХрпБ',  # Completely entertaining\n",
        "        'роЕро▒рпНрокрпБродрооро╛рой рокроЯроорпН',  # Awesome movie\n",
        "        'роЕро▒рпНрокрпБродрооро╛рой ро╡ро┐ро│роХрпНроХроХрпНроХро╛роЯрпНроЪро┐',  # Marvelous presentation\n",
        "        'роЪроХрпНродро┐ро╡ро╛ропрпНроирпНрод роироЯро┐рокрпНрокрпБ',  # Powerful acting\n",
        "        'родрпНро░ро┐ро▓рпНро▓ро┐роЩрпН роЕройрпБрокро╡роорпН',  # Thrilling experience\n",
        "        'ропродро╛ро░рпНродрпНродрооро╛рой роЪро┐родрпНродро░ро┐рокрпНрокрпБ',  # Realistic portrayal\n",
        "        'роЪроорпВроХ роЪрпЖропрпНродро┐ропрпБроЯройрпН',  # With social message\n",
        "        'рооро╛ро╕рпНроЯро░рпНрокрпАро╕рпН рокроЯроорпН',  # Masterpiece movie\n",
        "        'роТро╡рпНро╡рпКро░рпБ роХро╛роЯрпНроЪро┐ропрпБроорпН роЕро▒рпНрокрпБродроорпН',  # Every scene is wonderful\n",
        "    ]\n",
        "\n",
        "    tamil_negative = [\n",
        "        'роОройроХрпНроХрпБ роЗроирпНрод рокроЯроорпН рокро┐роЯро┐роХрпНроХро╡ро┐ро▓рпНро▓рпИ',  # I didn't like this movie\n",
        "        'роорпЛроЪрооро╛рой рокроЯроорпН',  # Bad movie\n",
        "        'роХродрпИ рооро┐роХро╡рпБроорпН рокро▓ро╡рпАройрооро╛роХ роЙро│рпНро│родрпБ',  # Story is very weak\n",
        "        'роЪро▓ро┐рокрпНрокро╛рой рооро▒рпНро▒рпБроорпН роирпАрогрпНроЯ рокроЯроорпН',  # Boring and long movie\n",
        "        'рокрогроорпН ро╡рпАрогро╛роХро┐ро╡ро┐роЯрпНроЯродрпБ',  # Money wasted\n",
        "        'роироЯро┐рокрпНрокрпБ роЪро░ро┐ропро┐ро▓рпНро▓рпИ',  # Acting didn't work\n",
        "        'роЗропроХрпНроХродрпНродро┐ро▓рпН роХрпБро▒рпИрокро╛роЯрпБ',  # Lack in direction\n",
        "        'роЗроЪрпИропрпБроорпН роиройрпНро▒ро╛роХ роЗро▓рпНро▓рпИ',  # Music is also not good\n",
        "        'рокро╛ро░рпНроХрпНроХ родроХрпБродро┐ропро▒рпНро▒родрпБ',  # Not worth watching\n",
        "        'роирпЗро░роорпН ро╡рпАрогро╛роХро┐ро╡ро┐роЯрпНроЯродрпБ',  # Time got wasted\n",
        "        'рокрпБродро┐родро╛роХ роОродрпБро╡рпБроорпН роЗро▓рпНро▓рпИ',  # Nothing new\n",
        "        'рокро┤рпИроп роХродрпИ роорпАрогрпНроЯрпБроорпН',  # Old story again\n",
        "        'роХрпБро┤роирпНродрпИроХро│рпБроХрпНроХрпБ роХро╛роЯрпНроЯ ро╡рпЗрогрпНроЯро╛роорпН',  # Don't show to children\n",
        "        'рооро┐роХро╡рпБроорпН роПрооро╛ро▒рпНро▒рооро│ро┐роХрпНроХро┐ро▒родрпБ',  # Very disappointing\n",
        "        'роЖроХрпНро╖ройрпН роХрпВроЯ ро╡рпАрогро╛ройродрпБ',  # Action also useless\n",
        "        'рокро╛роЯро▓рпНроХро│рпН роХро╛родрпБроХро│рпБроХрпНроХрпБ ро╡ро▓ро┐',  # Songs hurt the ears\n",
        "        'родро┐ро░рпБрокрпНрокроорпН роЗро▓рпНро▓рпИ',  # No twist\n",
        "        'ро╡ро┐ро╖рпБро╡ро▓рпН роОроГрокрпЖроХрпНроЯрпНро╕рпН роорпЛроЪроорпН',  # Poor visual effects\n",
        "        'роЙрогро░рпНроЪрпНроЪро┐рокрпВро░рпНро╡рооро╛роХ рокро▓ро╡рпАройроорпН',  # Emotionally weak\n",
        "        'роироХрпИроЪрпНроЪрпБро╡рпИ роЪрпБро╡рпИропро▒рпНро▒родрпБ',  # Comedy is bland\n",
        "        'роХроЯрпНроЯро╛ропрооро╛рой роХро╛родро▓рпН',  # Forced romance\n",
        "        'роОроирпНрод роЪрпЖропрпНродро┐ропрпБроорпН роЗро▓рпНро▓рпИ',  # No message\n",
        "        'рооро▒роирпНродрпБро╡ро┐роЯроХрпНроХрпВроЯро┐роп рокроЯроорпН',  # Forgettable movie\n",
        "        'ро╡рпАроЯрпНроЯро┐ро▓рпБроорпН рокро╛ро░рпНроХрпНроХ ро╡рпЗрогрпНроЯро╛роорпН',  # Don't watch even at home\n",
        "        'роХрпБроЯрпБроорпНрокродрпНродрпБроЯройрпН роЪроЩрпНроХроЯроорпН',  # Embarrassment with family\n",
        "        'родро┐ро░рпИроХрпНроХродрпИропро┐ро▓рпН родрпБро│рпИроХро│рпН',  # Holes in screenplay\n",
        "        'роироЯро┐рокрпНрокрпБ роЪрпЖропро▒рпНроХрпИропро╛роХ роЗро░рпБроирпНродродрпБ',  # Acting seemed artificial\n",
        "        'роТро│ро┐рокрпНрокродро┐ро╡рпБ роЪро░ро╛роЪро░ро┐',  # Average cinematography\n",
        "        'ро╡роЪройроЩрпНроХро│рпН роХрпБро┤роирпНродрпИродрпНродройрооро╛ройро╡рпИ',  # Dialogues are childish\n",
        "        'роХрпНро│рпИрооро╛роХрпНро╕рпН роПрооро╛ро▒рпНро▒рооро│ро┐роХрпНроХро┐ро▒родрпБ',  # Climax disappoints\n",
        "        'роЪро╕рпНрокрпЖройрпНро╕рпН роЗро▓рпНро▓ро╡рпЗ роЗро▓рпНро▓рпИ',  # No suspense at all\n",
        "        'роЕро┤ ро╡рпИрокрпНрокродро┐ро▓рпН родрпЛро▓рпНро╡ро┐',  # Failed to make cry\n",
        "        'роЪро┐ро░ро┐роХрпНроХ ро╡рпИроХрпНроХрпБроорпН роорпБропро▒рпНроЪро┐ родрпЛро▓рпНро╡ро┐',  # Failed attempt to make laugh\n",
        "        'роХро╛родро▓рпН роХродрпИ рокро┤рпИропродро╛роХ роЙро│рпНро│родрпБ',  # Love story is cliched\n",
        "        'рокро╛ро░рпНроХрпНроХрпБроорпНрокрпЛродрпБ родрпВроХрпНроХроорпН ро╡роирпНродродрпБ',  # Felt sleepy while watching\n",
        "        'роирпЗро░роорпН ро╡рпАрогро╛роХро┐ро╡ро┐роЯрпНроЯродрпБ',  # Time got wasted\n",
        "        'роХрпБро┤роирпНродрпИроХро│рпИ роЪро▓ро┐рокрпНрокроЯрпИропроЪрпН роЪрпЖропрпНропрпБроорпН',  # Will bore children\n",
        "        'родрооро┐ро┤рпН роЪро┐ройро┐рооро╛ро╡ро┐ройрпН роорпЛроЪрооро╛ройродрпБ',  # Worst of Tamil cinema\n",
        "        'родрпЛро▓рпНро╡ро┐ропроЯрпИропрпБроорпН рокроЯроорпН',  # Movie that will flop\n",
        "        'роТро░рпБ роорпБро▒рпИ роХрпВроЯ рокро╛ро░рпНроХрпНроХ ро╡рпЗрогрпНроЯро╛роорпН',  # Don't watch even once\n",
        "        'роорпБро▒рпНро▒ро┐ро▓рпБроорпН ро╡рпАрогро╛ройродрпБ',  # Completely useless\n",
        "        'роорпЛроЪрооро╛рой рокроЯроорпН',  # Poor quality movie\n",
        "        'роорпЛроЪрооро╛рой ро╡ро┐ро│роХрпНроХроХрпНроХро╛роЯрпНроЪро┐',  # Bad presentation\n",
        "        'рокро▓ро╡рпАройрооро╛рой роироЯро┐рокрпНрокрпБ',  # Weak acting\n",
        "        'родрпНро░ро┐ро▓рпН роЗро▓рпНро▓рпИ',  # No thrill\n",
        "        'роироЯрпИроорпБро▒рпИроХрпНроХрпБ рооро╛ро▒ро╛рой роЪро┐родрпНродро░ро┐рокрпНрокрпБ',  # Unrealistic portrayal\n",
        "        'рокропройро▒рпНро▒ роЪроорпВроХ роЪрпЖропрпНродро┐',  # Useless social message\n",
        "        'родрпЛро▓рпНро╡ро┐ропроЯрпИроирпНрод рокроЯроорпН',  # Failed movie\n",
        "        'роТро╡рпНро╡рпКро░рпБ роХро╛роЯрпНроЪро┐ропрпБроорпН роЪро▓ро┐рокрпНрокрпВроЯрпНроЯрпБроХро┐ро▒родрпБ',  # Every scene is boring\n",
        "    ]\n",
        "\n",
        "    tamil_data = pd.DataFrame({\n",
        "        'text': tamil_positive + tamil_negative,\n",
        "        'label': [1] * len(tamil_positive) + [0] * len(tamil_negative)\n",
        "    })\n",
        "\n",
        "    # Shuffle the data\n",
        "    hindi_data = hindi_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "    telugu_data = telugu_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "    tamil_data = tamil_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    # Print statistics\n",
        "    print(\"\\nЁЯУК Dataset Statistics:\")\n",
        "    print(f\"   Hindi: {len(hindi_data)} samples ({hindi_data['label'].sum()} positive, {len(hindi_data) - hindi_data['label'].sum()} negative)\")\n",
        "    print(f\"   Telugu: {len(telugu_data)} samples ({telugu_data['label'].sum()} positive, {len(telugu_data) - telugu_data['label'].sum()} negative)\")\n",
        "    print(f\"   Tamil: {len(tamil_data)} samples ({tamil_data['label'].sum()} positive, {len(tamil_data) - tamil_data['label'].sum()} negative)\")\n",
        "\n",
        "    return hindi_data, telugu_data, tamil_data\n",
        "\n",
        "hindi_test, telugu_test, tamil_test = create_sample_indic_data()\n",
        "\n",
        "print(f\"тЬЕ Hindi test samples: {len(hindi_test)}\")\n",
        "print(f\"тЬЕ Telugu test samples: {len(telugu_test)}\")\n",
        "print(f\"тЬЕ Tamil test samples: {len(tamil_test)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AMvvgJzwnt6"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# STEP 2: DATA PREPROCESSING\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 2: DATA PREPROCESSING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Minimal preprocessing for cross-lingual transfer\"\"\"\n",
        "    import re\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    # Remove extra whitespace\n",
        "    text = ' '.join(text.split())\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing\n",
        "print(\"ЁЯФз Applying minimal preprocessing...\")\n",
        "\n",
        "# Process English data\n",
        "train_texts = [preprocess_text(x['text']) for x in train_data]\n",
        "train_labels = [x['label'] for x in train_data]\n",
        "\n",
        "val_texts = [preprocess_text(x['text']) for x in val_data]\n",
        "val_labels = [x['label'] for x in val_data]\n",
        "\n",
        "# Process Indic data\n",
        "hindi_test['text'] = hindi_test['text'].apply(preprocess_text)\n",
        "telugu_test['text'] = telugu_test['text'].apply(preprocess_text)\n",
        "tamil_test['text'] = tamil_test['text'].apply(preprocess_text)\n",
        "\n",
        "print(\"тЬЕ Preprocessing complete\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 3: MODEL SELECTION & TOKENIZATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 3: MODEL SELECTION & TOKENIZATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Models to compare\n",
        "MODELS = {\n",
        "    'xlm-roberta-base': 'XLM-RoBERTa (Best cross-lingual)',\n",
        "    'bert-base-multilingual-cased': 'mBERT (Baseline)',\n",
        "    'ai4bharat/indic-bert': 'IndicBERT (Indic-focused)'\n",
        "}\n",
        "\n",
        "# Select model for this run (change to compare different models)\n",
        "MODEL_NAME = 'xlm-roberta-base'  # Change this to test other models\n",
        "\n",
        "print(f\"\\nЁЯдЦ Selected Model: {MODELS[MODEL_NAME]}\")\n",
        "print(f\"Model ID: {MODEL_NAME}\")\n",
        "\n",
        "# Load tokenizer with proper handling for IndicBERT\n",
        "print(\"ЁЯУж Loading tokenizer...\")\n",
        "if 'indic-bert' in MODEL_NAME:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "else:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Tokenize function\n",
        "def tokenize_data(texts, labels, max_length=256):\n",
        "    \"\"\"Tokenize text data\"\"\"\n",
        "    encodings = tokenizer(\n",
        "        texts,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    return encodings, labels\n",
        "\n",
        "# Tokenize datasets\n",
        "print(\"ЁЯФд Tokenizing datasets...\")\n",
        "train_encodings, train_labels_tensor = tokenize_data(train_texts, train_labels)\n",
        "val_encodings, val_labels_tensor = tokenize_data(val_texts, val_labels)\n",
        "\n",
        "# Create PyTorch datasets\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "train_dataset = SentimentDataset(train_encodings, train_labels)\n",
        "val_dataset = SentimentDataset(val_encodings, val_labels_tensor)\n",
        "\n",
        "print(f\"тЬЕ Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"тЬЕ Validation dataset size: {len(val_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Q-iQwi9wy6t"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================================\n",
        "# STEP 3: MODEL SELECTION & TOKENIZATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 3: MODEL SELECTION & TOKENIZATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Models to compare\n",
        "MODELS = {\n",
        "    'xlm-roberta-base': 'XLM-RoBERTa (Best cross-lingual)',\n",
        "    'bert-base-multilingual-cased': 'mBERT (Baseline)',\n",
        "    'ai4bharat/indic-bert': 'IndicBERT (Indic-focused)'\n",
        "}\n",
        "\n",
        "# Select model for this run (change to compare different models)\n",
        "MODEL_NAME = 'xlm-roberta-base'  # Change this to test other models\n",
        "\n",
        "print(f\"\\nЁЯдЦ Selected Model: {MODELS[MODEL_NAME]}\")\n",
        "print(f\"Model ID: {MODEL_NAME}\")\n",
        "\n",
        "# Load tokenizer with proper handling for IndicBERT\n",
        "print(\"ЁЯУж Loading tokenizer...\")\n",
        "if 'indic-bert' in MODEL_NAME:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "else:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Tokenize function\n",
        "def tokenize_data(texts, labels, max_length=256):\n",
        "    \"\"\"Tokenize text data\"\"\"\n",
        "    encodings = tokenizer(\n",
        "        texts,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    return encodings, labels\n",
        "\n",
        "# Tokenize datasets\n",
        "print(\"ЁЯФд Tokenizing datasets...\")\n",
        "train_encodings, train_labels_tensor = tokenize_data(train_texts, train_labels)\n",
        "val_encodings, val_labels_tensor = tokenize_data(val_texts, val_labels)\n",
        "\n",
        "# Create PyTorch datasets\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "train_dataset = SentimentDataset(train_encodings, train_labels)\n",
        "val_dataset = SentimentDataset(val_encodings, val_labels_tensor)\n",
        "\n",
        "print(f\"тЬЕ Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"тЬЕ Validation dataset size: {len(val_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nrUThjZIxbrp"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# STEP 4: MODEL FINE-TUNING (English Only) тАФ VERSION SAFE\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 4: FINE-TUNING ON ENGLISH DATA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Load model\n",
        "print(f\"ЁЯФз Loading {MODEL_NAME}...\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=2\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "# Training arguments (OLD + NEW transformers compatible)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=f'./results_{MODEL_NAME.replace(\"/\", \"_\")}',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=32,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=100,\n",
        "    eval_steps=500,\n",
        "    save_steps=500,\n",
        "    report_to=\"none\",\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# Metrics computation\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=1)\n",
        "\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    f1 = f1_score(labels, predictions, average=\"weighted\")\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": acc,\n",
        "        \"f1\": f1\n",
        "    }\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Train model\n",
        "print(\"ЁЯЪА Starting training...\")\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate on English validation set\n",
        "print(\"\\nЁЯУК Evaluating on English validation set...\")\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "print(f\"English Validation Accuracy: {eval_results['eval_accuracy']:.4f}\")\n",
        "print(f\"English Validation F1: {eval_results['eval_f1']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgRkCb0eyjFW"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# STEP 5: ZERO-SHOT CROSS-LINGUAL EVALUATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 5: ZERO-SHOT CROSS-LINGUAL EVALUATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def evaluate_on_indic(model, tokenizer, df, language_name):\n",
        "    \"\"\"Evaluate model on Indic language data\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    texts = df['text'].tolist()\n",
        "    true_labels = df['label'].tolist()\n",
        "\n",
        "    # Tokenize\n",
        "    encodings = tokenizer(\n",
        "        texts,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=256,\n",
        "        return_tensors='pt'\n",
        "    ).to(device)\n",
        "\n",
        "    # Predict\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**encodings)\n",
        "        predictions = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
        "\n",
        "    # Calculate metrics\n",
        "    acc = accuracy_score(true_labels, predictions)\n",
        "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
        "\n",
        "    print(f\"\\n{language_name} Results:\")\n",
        "    print(f\"  Accuracy: {acc:.4f}\")\n",
        "    print(f\"  F1-Score: {f1:.4f}\")\n",
        "    print(f\"\\nClassification Report:\")\n",
        "    print(classification_report(true_labels, predictions,\n",
        "                                target_names=['Negative', 'Positive']))\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(true_labels, predictions)\n",
        "\n",
        "    return {\n",
        "        'language': language_name,\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'predictions': predictions,\n",
        "        'true_labels': true_labels,\n",
        "        'confusion_matrix': cm\n",
        "    }\n",
        "\n",
        "# Evaluate on all Indic languages\n",
        "results = {}\n",
        "\n",
        "print(\"\\nЁЯФН Zero-Shot Evaluation Results:\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "results['hindi'] = evaluate_on_indic(model, tokenizer, hindi_test, \"Hindi\")\n",
        "results['telugu'] = evaluate_on_indic(model, tokenizer, telugu_test, \"Telugu\")\n",
        "results['tamil'] = evaluate_on_indic(model, tokenizer, tamil_test, \"Tamil\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 6: VISUALIZATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 6: VISUALIZATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# 6.1 Performance Comparison Across Languages\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "languages = ['Hindi', 'Telugu', 'Tamil']\n",
        "accuracies = [results['hindi']['accuracy'],\n",
        "              results['telugu']['accuracy'],\n",
        "              results['tamil']['accuracy']]\n",
        "f1_scores = [results['hindi']['f1'],\n",
        "             results['telugu']['f1'],\n",
        "             results['tamil']['f1']]\n",
        "\n",
        "# Accuracy plot\n",
        "axes[0].bar(languages, accuracies, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
        "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
        "axes[0].set_title('Zero-Shot Accuracy by Language', fontsize=14, fontweight='bold')\n",
        "axes[0].set_ylim([0, 1])\n",
        "for i, v in enumerate(accuracies):\n",
        "    axes[0].text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold')\n",
        "\n",
        "# F1-Score plot\n",
        "axes[1].bar(languages, f1_scores, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
        "axes[1].set_ylabel('F1-Score', fontsize=12)\n",
        "axes[1].set_title('Zero-Shot F1-Score by Language', fontsize=14, fontweight='bold')\n",
        "axes[1].set_ylim([0, 1])\n",
        "for i, v in enumerate(f1_scores):\n",
        "    axes[1].text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('zero_shot_performance.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# 6.2 Confusion Matrices\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "for idx, (lang_key, lang_name) in enumerate([('hindi', 'Hindi'),\n",
        "                                               ('telugu', 'Telugu'),\n",
        "                                               ('tamil', 'Tamil')]):\n",
        "    cm = results[lang_key]['confusion_matrix']\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Negative', 'Positive'],\n",
        "                yticklabels=['Negative', 'Positive'],\n",
        "                ax=axes[idx])\n",
        "    axes[idx].set_title(f'{lang_name} Confusion Matrix', fontweight='bold')\n",
        "    axes[idx].set_ylabel('True Label')\n",
        "    axes[idx].set_xlabel('Predicted Label')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OZb7ZgwFIvVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "8YfjByUKVaTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 7: ENHANCED ERROR ANALYSIS WITH TAXONOMY\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 7: ENHANCED ERROR ANALYSIS WITH TAXONOMY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def categorize_error(text, true_label, predicted_label):\n",
        "    \"\"\"Categorize errors into specific types\"\"\"\n",
        "    text_lower = text.lower()\n",
        "\n",
        "    # Negation detection (simple heuristic)\n",
        "    negation_words_en = ['not', 'never', 'no', 'none', 'neither']\n",
        "    negation_words_hi = ['рдирд╣реАрдВ', 'рдХрднреА рдирд╣реАрдВ', 'рдмрд┐рд▓реНрдХреБрд▓ рдирд╣реАрдВ']\n",
        "    negation_words_te = ['р░Хр░╛р░жр▒Б', 'р░▓р▒Зр░жр▒Б']\n",
        "    negation_words_ta = ['роЗро▓рпНро▓рпИ', 'роЗро▓рпНро▓ро╛род']\n",
        "\n",
        "    all_negations = negation_words_en + negation_words_hi + negation_words_te + negation_words_ta\n",
        "\n",
        "    has_negation = any(neg in text_lower for neg in all_negations)\n",
        "\n",
        "    # Code-mixing detection (contains both Indic and English)\n",
        "    has_english = bool(re.search(r'[a-zA-Z]{3,}', text))\n",
        "    has_indic = bool(re.search(r'[\\u0900-\\u097F\\u0C00-\\u0C7F\\u0B80-\\u0BFF]', text))\n",
        "    is_code_mixed = has_english and has_indic\n",
        "\n",
        "    # Intensity words\n",
        "    intensity_words = ['рдмрд╣реБрдд', 'рооро┐роХро╡рпБроорпН', 'р░Ър░╛р░▓р░╛', 'very', 'really', 'extremely']\n",
        "    has_intensity = any(word in text for word in intensity_words)\n",
        "\n",
        "    # Length-based\n",
        "    is_short = len(text.split()) < 5\n",
        "    is_long = len(text.split()) > 20\n",
        "\n",
        "    # Categorize\n",
        "    if has_negation and true_label != predicted_label:\n",
        "        return 'Negation'\n",
        "    elif is_code_mixed:\n",
        "        return 'Code-mixed'\n",
        "    elif is_short:\n",
        "        return 'Short text'\n",
        "    elif is_long:\n",
        "        return 'Long text'\n",
        "    elif has_intensity:\n",
        "        return 'Intensity mismatch'\n",
        "    else:\n",
        "        return 'Other'\n",
        "\n",
        "def enhanced_error_analysis(df, predictions, language_name):\n",
        "    \"\"\"Enhanced error analysis with categorization\"\"\"\n",
        "    df_copy = df.copy()\n",
        "    df_copy['predicted'] = predictions\n",
        "    df_copy['correct'] = df_copy['label'] == df_copy['predicted']\n",
        "\n",
        "    errors = df_copy[~df_copy['correct']]\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"{language_name} ENHANCED ERROR ANALYSIS\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Total errors: {len(errors)}/{len(df_copy)} ({len(errors)/len(df_copy)*100:.1f}%)\")\n",
        "\n",
        "    if len(errors) > 0:\n",
        "        # Categorize errors\n",
        "        error_categories = {}\n",
        "        for idx, row in errors.iterrows():\n",
        "            category = categorize_error(row['text'], row['label'], row['predicted'])\n",
        "            if category not in error_categories:\n",
        "                error_categories[category] = []\n",
        "            error_categories[category].append({\n",
        "                'text': row['text'],\n",
        "                'true': row['label'],\n",
        "                'pred': row['predicted']\n",
        "            })\n",
        "\n",
        "        # Print error distribution\n",
        "        print(f\"\\nЁЯУК Error Distribution by Category:\")\n",
        "        print(\"-\" * 80)\n",
        "        for category, items in sorted(error_categories.items(), key=lambda x: len(x[1]), reverse=True):\n",
        "            percentage = len(items) / len(errors) * 100\n",
        "            print(f\"  {category:20s}: {len(items):3d} errors ({percentage:5.1f}%)\")\n",
        "\n",
        "        # Show examples from each category\n",
        "        print(f\"\\nЁЯФН Example Errors by Category:\")\n",
        "        print(\"-\" * 80)\n",
        "        for category, items in error_categories.items():\n",
        "            if len(items) > 0:\n",
        "                print(f\"\\n  [{category}]\")\n",
        "                example = items[0]\n",
        "                print(f\"    Text: {example['text'][:80]}...\")\n",
        "                print(f\"    True: {'Positive' if example['true']==1 else 'Negative'} | \"\n",
        "                      f\"Pred: {'Positive' if example['pred']==1 else 'Negative'}\")\n",
        "\n",
        "        return errors, error_categories\n",
        "\n",
        "    return errors, {}\n",
        "\n",
        "# Run enhanced error analysis for all languages\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"RUNNING ENHANCED ERROR ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "hindi_errors, hindi_categories = enhanced_error_analysis(\n",
        "    hindi_test, results['hindi']['predictions'], \"HINDI\"\n",
        ")\n",
        "telugu_errors, telugu_categories = enhanced_error_analysis(\n",
        "    telugu_test, results['telugu']['predictions'], \"TELUGU\"\n",
        ")\n",
        "tamil_errors, tamil_categories = enhanced_error_analysis(\n",
        "    tamil_test, results['tamil']['predictions'], \"TAMIL\"\n",
        ")\n",
        "\n",
        "# Aggregate error categories across languages\n",
        "all_categories = set()\n",
        "for cats in [hindi_categories, telugu_categories, tamil_categories]:\n",
        "    all_categories.update(cats.keys())\n",
        "\n",
        "# Create error taxonomy visualization\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "category_counts = {cat: [] for cat in all_categories}\n",
        "languages = ['Hindi', 'Telugu', 'Tamil']\n",
        "\n",
        "for cats, lang in [(hindi_categories, 'Hindi'), (telugu_categories, 'Telugu'),\n",
        "                    (tamil_categories, 'Tamil')]:\n",
        "    for cat in all_categories:\n",
        "        count = len(cats.get(cat, []))\n",
        "        category_counts[cat].append(count)\n",
        "\n",
        "x = np.arange(len(all_categories))\n",
        "width = 0.25\n",
        "\n",
        "for i, lang in enumerate(languages):\n",
        "    counts = [category_counts[cat][i] for cat in all_categories]\n",
        "    ax.bar(x + i*width, counts, width, label=lang)\n",
        "\n",
        "ax.set_ylabel('Number of Errors', fontsize=12, fontweight='bold')\n",
        "ax.set_title('Error Taxonomy: Distribution Across Languages', fontsize=14, fontweight='bold')\n",
        "ax.set_xticks(x + width)\n",
        "ax.set_xticklabels(list(all_categories), rotation=45, ha='right')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('error_taxonomy.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nтЬЕ Error taxonomy saved to 'error_taxonomy.png'\")\n",
        "\n",
        "# Save error analysis\n",
        "error_analysis_results = {\n",
        "    'hindi': {cat: len(items) for cat, items in hindi_categories.items()},\n",
        "    'telugu': {cat: len(items) for cat, items in telugu_categories.items()},\n",
        "    'tamil': {cat: len(items) for cat, items in tamil_categories.items()}\n",
        "}\n",
        "\n",
        "with open('error_taxonomy.json', 'w') as f:\n",
        "    json.dump(error_analysis_results, f, indent=2)\n",
        "\n",
        "print(\"ЁЯТ╛ Error taxonomy saved to 'error_taxonomy.json'\")\n"
      ],
      "metadata": {
        "id": "eMfuLltdI3ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================================================\n",
        "# STEP 8: SUMMARY & INSIGHTS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 8: RESEARCH FINDINGS SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "summary = f\"\"\"\n",
        "тХФтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХЧ\n",
        "тХС           CROSS-LINGUAL SENTIMENT ANALYSIS RESULTS                    тХС\n",
        "тХЪтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХЭ\n",
        "\n",
        "ЁЯУК MODEL: {MODELS[MODEL_NAME]}\n",
        "тФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБ\n",
        "\n",
        "тЬЕ ZERO-SHOT TRANSFER RESULTS:\n",
        "\n",
        "   Language      Accuracy    F1-Score\n",
        "   тФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФА\n",
        "   Hindi         {results['hindi']['accuracy']:.4f}      {results['hindi']['f1']:.4f}\n",
        "   Telugu        {results['telugu']['accuracy']:.4f}      {results['telugu']['f1']:.4f}\n",
        "   Tamil         {results['tamil']['accuracy']:.4f}      {results['tamil']['f1']:.4f}\n",
        "\n",
        "ЁЯФН KEY INSIGHTS:\n",
        "\n",
        "1. Zero-shot transfer {'WORKS' if min(accuracies) > 0.5 else 'NEEDS IMPROVEMENT'}\n",
        "   тЖТ Model trained only on English can classify Indic languages\n",
        "   тЖТ {'Strong' if min(accuracies) > 0.7 else 'Moderate' if min(accuracies) > 0.5 else 'Weak'} cross-lingual alignment\n",
        "\n",
        "2. Language Performance Ranking:\n",
        "   тЖТ Best: {languages[np.argmax(accuracies)]} ({max(accuracies):.3f})\n",
        "   тЖТ Worst: {languages[np.argmin(accuracies)]} ({min(accuracies):.3f})\n",
        "\n",
        "3. Linguistic Insights:\n",
        "   тЖТ {'Indo-Aryan (Hindi) performs better than Dravidian' if results['hindi']['accuracy'] > max(results['telugu']['accuracy'], results['tamil']['accuracy']) else 'Performance varies across language families'}\n",
        "\n",
        "тФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБ\n",
        "\n",
        "ЁЯУМ NEXT STEPS:\n",
        "   1. Compare with other models (mBERT, IndicBERT)\n",
        "   2. Add few-shot fine-tuning experiments\n",
        "   3. Perform layer-wise transfer analysis\n",
        "   4. Expand error taxonomy with more examples\n",
        "   5. Test on larger datasets\n",
        "\n",
        "тХЪтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХРтХЭ\n",
        "\"\"\"\n",
        "\n",
        "print(summary)\n",
        "\n",
        "# Save results to JSON\n",
        "results_summary = {\n",
        "    'model': MODEL_NAME,\n",
        "    'english_validation': {\n",
        "        'accuracy': float(eval_results['eval_accuracy']),\n",
        "        'f1': float(eval_results['eval_f1'])\n",
        "    },\n",
        "    'zero_shot': {\n",
        "        'hindi': {'accuracy': float(results['hindi']['accuracy']),\n",
        "                  'f1': float(results['hindi']['f1'])},\n",
        "        'telugu': {'accuracy': float(results['telugu']['accuracy']),\n",
        "                   'f1': float(results['telugu']['f1'])},\n",
        "        'tamil': {'accuracy': float(results['tamil']['accuracy']),\n",
        "                  'f1': float(results['tamil']['f1'])}\n",
        "    }\n",
        "}\n",
        "\n",
        "with open('results_summary.json', 'w') as f:\n",
        "    json.dump(results_summary, f, indent=2)\n",
        "\n",
        "print(\"\\nЁЯТ╛ Results saved to 'results_summary.json'\")\n"
      ],
      "metadata": {
        "id": "I4lv8Uf4Jb3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================================================\n",
        "# STEP 9: FEW-SHOT LEARNING EXPERIMENTS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 9: FEW-SHOT LEARNING EXPERIMENTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\"\"\n",
        "ЁЯОп EXPERIMENT SETUP:\n",
        "   We'll train with small amounts of target language data to see how\n",
        "   quickly the model adapts compared to zero-shot transfer.\n",
        "\n",
        "   Training sizes: 10, 25, 50, 75 samples per language\n",
        "\"\"\")\n",
        "\n",
        "# Store zero-shot results for comparison\n",
        "zero_shot_results = {\n",
        "    'hindi': results['hindi']['accuracy'],\n",
        "    'telugu': results['telugu']['accuracy'],\n",
        "    'tamil': results['tamil']['accuracy']\n",
        "}\n",
        "\n",
        "# Few-shot sample sizes to test\n",
        "FEW_SHOT_SIZES = [10, 25, 50, 75]\n",
        "\n",
        "# Store results for all experiments\n",
        "few_shot_results = {\n",
        "    'hindi': {'sizes': [], 'accuracies': [], 'f1_scores': []},\n",
        "    'telugu': {'sizes': [], 'accuracies': [], 'f1_scores': []},\n",
        "    'tamil': {'sizes': [], 'accuracies': [], 'f1_scores': []}\n",
        "}\n",
        "\n",
        "def create_few_shot_dataset(indic_df, n_samples, language_name):\n",
        "    \"\"\"Create a few-shot training dataset\"\"\"\n",
        "    # Ensure balanced sampling\n",
        "    n_per_class = n_samples // 2\n",
        "\n",
        "    positive_samples = indic_df[indic_df['label'] == 1].sample(n=n_per_class, random_state=42)\n",
        "    negative_samples = indic_df[indic_df['label'] == 0].sample(n=n_per_class, random_state=42)\n",
        "\n",
        "    few_shot_df = pd.concat([positive_samples, negative_samples]).sample(frac=1, random_state=42)\n",
        "\n",
        "    print(f\"\\nЁЯУж Created {language_name} few-shot dataset:\")\n",
        "    print(f\"   Total samples: {len(few_shot_df)}\")\n",
        "    print(f\"   Positive: {few_shot_df['label'].sum()}, Negative: {len(few_shot_df) - few_shot_df['label'].sum()}\")\n",
        "\n",
        "    return few_shot_df\n",
        "\n",
        "def train_few_shot_model(train_texts_en, train_labels_en, indic_texts, indic_labels,\n",
        "                         model_name, n_epochs=3):\n",
        "    \"\"\"Train model with English + few Indic samples\"\"\"\n",
        "\n",
        "    # Combine English and Indic data\n",
        "    combined_texts = train_texts_en + indic_texts\n",
        "    combined_labels = train_labels_en + indic_labels\n",
        "\n",
        "    print(f\"   Combined training size: {len(combined_texts)} samples\")\n",
        "    print(f\"   English: {len(train_texts_en)}, Indic: {len(indic_texts)}\")\n",
        "\n",
        "    # Tokenize combined data\n",
        "    encodings = tokenizer(\n",
        "        combined_texts,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=256,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    # Create dataset\n",
        "    combined_dataset = SentimentDataset(encodings, combined_labels)\n",
        "\n",
        "    # Load fresh model\n",
        "    few_shot_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=2\n",
        "    ).to(device)\n",
        "\n",
        "    # Training arguments (fewer epochs for few-shot)\n",
        "    few_shot_args = TrainingArguments(\n",
        "        output_dir=f'./few_shot_results',\n",
        "        num_train_epochs=n_epochs,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=32,\n",
        "        warmup_steps=100,\n",
        "        weight_decay=0.01,\n",
        "        logging_steps=50,\n",
        "        eval_strategy=\"no\",\n",
        "        save_strategy=\"no\",\n",
        "        report_to=\"none\",\n",
        "        seed=42\n",
        "    )\n",
        "\n",
        "    # Train\n",
        "    few_shot_trainer = Trainer(\n",
        "        model=few_shot_model,\n",
        "        args=few_shot_args,\n",
        "        train_dataset=combined_dataset,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    few_shot_trainer.train()\n",
        "\n",
        "    return few_shot_model\n",
        "\n",
        "# Run few-shot experiments for each language\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"RUNNING FEW-SHOT EXPERIMENTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# We'll focus on Hindi for detailed few-shot analysis\n",
        "# (You can extend this to Telugu and Tamil)\n",
        "\n",
        "print(\"\\nЁЯФм HINDI FEW-SHOT EXPERIMENTS\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for n_samples in FEW_SHOT_SIZES:\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Training with {n_samples} Hindi samples\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    # Create few-shot training data\n",
        "    hindi_few_shot = create_few_shot_dataset(hindi_test, n_samples, \"Hindi\")\n",
        "\n",
        "    # Prepare for training (use only non-test samples)\n",
        "    hindi_train_texts = hindi_few_shot['text'].apply(preprocess_text).tolist()\n",
        "    hindi_train_labels = hindi_few_shot['label'].tolist()\n",
        "\n",
        "    # Train model\n",
        "    print(\"\\nЁЯЪА Training few-shot model...\")\n",
        "    few_shot_model_hindi = train_few_shot_model(\n",
        "        train_texts[:2000],  # Use subset of English data for faster training\n",
        "        train_labels[:2000],\n",
        "        hindi_train_texts,\n",
        "        hindi_train_labels,\n",
        "        MODEL_NAME,\n",
        "        n_epochs=2\n",
        "    )\n",
        "\n",
        "    # Evaluate on remaining Hindi samples (held-out test set)\n",
        "    # Create held-out test set (samples not used in training)\n",
        "    hindi_test_indices = set(hindi_test.index) - set(hindi_few_shot.index)\n",
        "    hindi_test_holdout = hindi_test.loc[list(hindi_test_indices)]\n",
        "\n",
        "    print(f\"\\nЁЯУК Evaluating on {len(hindi_test_holdout)} held-out Hindi samples...\")\n",
        "\n",
        "    # Evaluate\n",
        "    eval_result = evaluate_on_indic(few_shot_model_hindi, tokenizer,\n",
        "                                   hindi_test_holdout, f\"Hindi ({n_samples} shots)\")\n",
        "\n",
        "    # Store results\n",
        "    few_shot_results['hindi']['sizes'].append(n_samples)\n",
        "    few_shot_results['hindi']['accuracies'].append(eval_result['accuracy'])\n",
        "    few_shot_results['hindi']['f1_scores'].append(eval_result['f1'])\n",
        "\n",
        "    print(f\"\\nтЬЕ Few-shot ({n_samples} samples) Accuracy: {eval_result['accuracy']:.4f}\")\n",
        "    print(f\"тЬЕ Few-shot ({n_samples} samples) F1-Score: {eval_result['f1']:.4f}\")\n",
        "    print(f\"ЁЯУИ Improvement over zero-shot: {(eval_result['accuracy'] - zero_shot_results['hindi'])*100:.2f}%\")\n",
        "\n",
        "    # Clean up\n",
        "    del few_shot_model_hindi\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "f775EW-SWoct",
        "outputId": "873b907a-ede4-4836-e95c-783004e3c426"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 9: FEW-SHOT LEARNING EXPERIMENTS\n",
            "================================================================================\n",
            "\n",
            "ЁЯОп EXPERIMENT SETUP:\n",
            "   We'll train with small amounts of target language data to see how\n",
            "   quickly the model adapts compared to zero-shot transfer.\n",
            "\n",
            "   Training sizes: 10, 25, 50, 75 samples per language\n",
            "\n",
            "\n",
            "================================================================================\n",
            "RUNNING FEW-SHOT EXPERIMENTS\n",
            "================================================================================\n",
            "\n",
            "ЁЯФм HINDI FEW-SHOT EXPERIMENTS\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "================================================================================\n",
            "Training with 10 Hindi samples\n",
            "================================================================================\n",
            "\n",
            "ЁЯУж Created Hindi few-shot dataset:\n",
            "   Total samples: 10\n",
            "   Positive: 5, Negative: 5\n",
            "\n",
            "ЁЯЪА Training few-shot model...\n",
            "   Combined training size: 2010 samples\n",
            "   English: 2000, Indic: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='252' max='252' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [252/252 02:58, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.693600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.650100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.494200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.341600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ЁЯУК Evaluating on 89 held-out Hindi samples...\n",
            "\n",
            "Hindi (10 shots) Results:\n",
            "  Accuracy: 0.7416\n",
            "  F1-Score: 0.7299\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.67      0.95      0.79        44\n",
            "    Positive       0.92      0.53      0.68        45\n",
            "\n",
            "    accuracy                           0.74        89\n",
            "   macro avg       0.79      0.74      0.73        89\n",
            "weighted avg       0.80      0.74      0.73        89\n",
            "\n",
            "\n",
            "тЬЕ Few-shot (10 samples) Accuracy: 0.7416\n",
            "тЬЕ Few-shot (10 samples) F1-Score: 0.7299\n",
            "ЁЯУИ Improvement over zero-shot: -10.69%\n",
            "\n",
            "================================================================================\n",
            "Training with 25 Hindi samples\n",
            "================================================================================\n",
            "\n",
            "ЁЯУж Created Hindi few-shot dataset:\n",
            "   Total samples: 24\n",
            "   Positive: 12, Negative: 12\n",
            "\n",
            "ЁЯЪА Training few-shot model...\n",
            "   Combined training size: 2024 samples\n",
            "   English: 2000, Indic: 24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='254' max='254' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [254/254 02:59, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.693500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.660000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.594900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.454000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.367600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ЁЯУК Evaluating on 75 held-out Hindi samples...\n",
            "\n",
            "Hindi (25 shots) Results:\n",
            "  Accuracy: 0.8000\n",
            "  F1-Score: 0.7974\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.74      0.92      0.82        37\n",
            "    Positive       0.90      0.68      0.78        38\n",
            "\n",
            "    accuracy                           0.80        75\n",
            "   macro avg       0.82      0.80      0.80        75\n",
            "weighted avg       0.82      0.80      0.80        75\n",
            "\n",
            "\n",
            "тЬЕ Few-shot (25 samples) Accuracy: 0.8000\n",
            "тЬЕ Few-shot (25 samples) F1-Score: 0.7974\n",
            "ЁЯУИ Improvement over zero-shot: -4.85%\n",
            "\n",
            "================================================================================\n",
            "Training with 50 Hindi samples\n",
            "================================================================================\n",
            "\n",
            "ЁЯУж Created Hindi few-shot dataset:\n",
            "   Total samples: 50\n",
            "   Positive: 25, Negative: 25\n",
            "\n",
            "ЁЯЪА Training few-shot model...\n",
            "   Combined training size: 2050 samples\n",
            "   English: 2000, Indic: 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='258' max='258' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [258/258 03:01, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.689000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.620800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.508600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.431500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.314100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ЁЯУК Evaluating on 49 held-out Hindi samples...\n",
            "\n",
            "Hindi (50 shots) Results:\n",
            "  Accuracy: 0.7551\n",
            "  F1-Score: 0.7520\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.70      0.88      0.78        24\n",
            "    Positive       0.84      0.64      0.73        25\n",
            "\n",
            "    accuracy                           0.76        49\n",
            "   macro avg       0.77      0.76      0.75        49\n",
            "weighted avg       0.77      0.76      0.75        49\n",
            "\n",
            "\n",
            "тЬЕ Few-shot (50 samples) Accuracy: 0.7551\n",
            "тЬЕ Few-shot (50 samples) F1-Score: 0.7520\n",
            "ЁЯУИ Improvement over zero-shot: -9.34%\n",
            "\n",
            "================================================================================\n",
            "Training with 75 Hindi samples\n",
            "================================================================================\n",
            "\n",
            "ЁЯУж Created Hindi few-shot dataset:\n",
            "   Total samples: 74\n",
            "   Positive: 37, Negative: 37\n",
            "\n",
            "ЁЯЪА Training few-shot model...\n",
            "   Combined training size: 2074 samples\n",
            "   English: 2000, Indic: 74\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='96' max='260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 96/260 01:06 < 01:56, 1.41 it/s, Epoch 0.73/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.688700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================================================\n",
        "# STEP 10: FEW-SHOT LEARNING CURVES & ANALYSIS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 10: FEW-SHOT LEARNING CURVES & ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Create comprehensive visualization\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# 10.1 Learning Curve - Accuracy\n",
        "ax1 = axes[0, 0]\n",
        "sizes_with_zero = [0] + few_shot_results['hindi']['sizes']\n",
        "accs_with_zero = [zero_shot_results['hindi']] + few_shot_results['hindi']['accuracies']\n",
        "\n",
        "ax1.plot(sizes_with_zero, accs_with_zero, marker='o', linewidth=2,\n",
        "         markersize=10, color='#2E86AB', label='Hindi')\n",
        "ax1.axhline(y=zero_shot_results['hindi'], color='red', linestyle='--',\n",
        "            alpha=0.5, label='Zero-shot baseline')\n",
        "ax1.fill_between(sizes_with_zero, zero_shot_results['hindi'], accs_with_zero,\n",
        "                  alpha=0.2, color='#2E86AB')\n",
        "ax1.set_xlabel('Number of Training Samples', fontsize=12, fontweight='bold')\n",
        "ax1.set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
        "ax1.set_title('Few-Shot Learning Curve: Hindi', fontsize=14, fontweight='bold')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.legend(fontsize=10)\n",
        "ax1.set_ylim([0, 1])\n",
        "\n",
        "# Add value labels on points\n",
        "for i, (x, y) in enumerate(zip(sizes_with_zero, accs_with_zero)):\n",
        "    ax1.annotate(f'{y:.3f}', (x, y), textcoords=\"offset points\",\n",
        "                xytext=(0,10), ha='center', fontsize=9, fontweight='bold')\n",
        "\n",
        "# 10.2 Learning Curve - F1 Score\n",
        "ax2 = axes[0, 1]\n",
        "f1_with_zero = [results['hindi']['f1']] + few_shot_results['hindi']['f1_scores']\n",
        "\n",
        "ax2.plot(sizes_with_zero, f1_with_zero, marker='s', linewidth=2,\n",
        "         markersize=10, color='#A23B72', label='Hindi F1')\n",
        "ax2.axhline(y=results['hindi']['f1'], color='red', linestyle='--',\n",
        "            alpha=0.5, label='Zero-shot F1 baseline')\n",
        "ax2.fill_between(sizes_with_zero, results['hindi']['f1'], f1_with_zero,\n",
        "                  alpha=0.2, color='#A23B72')\n",
        "ax2.set_xlabel('Number of Training Samples', fontsize=12, fontweight='bold')\n",
        "ax2.set_ylabel('F1-Score', fontsize=12, fontweight='bold')\n",
        "ax2.set_title('Few-Shot Learning Curve: F1-Score', fontsize=14, fontweight='bold')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.legend(fontsize=10)\n",
        "ax2.set_ylim([0, 1])\n",
        "\n",
        "# Add value labels\n",
        "for i, (x, y) in enumerate(zip(sizes_with_zero, f1_with_zero)):\n",
        "    ax2.annotate(f'{y:.3f}', (x, y), textcoords=\"offset points\",\n",
        "                xytext=(0,10), ha='center', fontsize=9, fontweight='bold')\n",
        "\n",
        "# 10.3 Improvement over Zero-Shot\n",
        "ax3 = axes[1, 0]\n",
        "improvements = [(acc - zero_shot_results['hindi']) * 100\n",
        "                for acc in few_shot_results['hindi']['accuracies']]\n",
        "\n",
        "bars = ax3.bar(few_shot_results['hindi']['sizes'], improvements,\n",
        "               color=['#06A77D', '#F77F00', '#D62828', '#023047'],\n",
        "               edgecolor='black', linewidth=1.5)\n",
        "ax3.set_xlabel('Number of Training Samples', fontsize=12, fontweight='bold')\n",
        "ax3.set_ylabel('Improvement over Zero-Shot (%)', fontsize=12, fontweight='bold')\n",
        "ax3.set_title('Absolute Improvement in Accuracy', fontsize=14, fontweight='bold')\n",
        "ax3.grid(True, alpha=0.3, axis='y')\n",
        "ax3.axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
        "\n",
        "# Add value labels on bars\n",
        "for i, (bar, val) in enumerate(zip(bars, improvements)):\n",
        "    height = bar.get_height()\n",
        "    ax3.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'+{val:.1f}%' if val > 0 else f'{val:.1f}%',\n",
        "            ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "\n",
        "# 10.4 Sample Efficiency Analysis\n",
        "ax4 = axes[1, 1]\n",
        "\n",
        "# Calculate samples needed to reach certain accuracy thresholds\n",
        "thresholds = [0.70, 0.75, 0.80, 0.85]\n",
        "samples_needed = []\n",
        "\n",
        "for threshold in thresholds:\n",
        "    # Find minimum samples needed to reach threshold\n",
        "    reached = False\n",
        "    for size, acc in zip(sizes_with_zero, accs_with_zero):\n",
        "        if acc >= threshold:\n",
        "            samples_needed.append(size)\n",
        "            reached = True\n",
        "            break\n",
        "    if not reached:\n",
        "        samples_needed.append(None)\n",
        "\n",
        "# Create bar chart\n",
        "valid_thresholds = [t for t, s in zip(thresholds, samples_needed) if s is not None]\n",
        "valid_samples = [s for s in samples_needed if s is not None]\n",
        "\n",
        "if valid_samples:\n",
        "    bars2 = ax4.barh([f'{t*100:.0f}%' for t in valid_thresholds], valid_samples,\n",
        "                     color='#F18F01', edgecolor='black', linewidth=1.5)\n",
        "    ax4.set_xlabel('Samples Required', fontsize=12, fontweight='bold')\n",
        "    ax4.set_ylabel('Target Accuracy', fontsize=12, fontweight='bold')\n",
        "    ax4.set_title('Sample Efficiency: Samples to Reach Accuracy',\n",
        "                  fontsize=14, fontweight='bold')\n",
        "    ax4.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "    # Add value labels\n",
        "    for i, (bar, val) in enumerate(zip(bars2, valid_samples)):\n",
        "        ax4.text(val, bar.get_y() + bar.get_height()/2.,\n",
        "                f'  {val} samples',\n",
        "                ha='left', va='center', fontsize=10, fontweight='bold')\n",
        "else:\n",
        "    ax4.text(0.5, 0.5, 'Accuracy thresholds\\nnot reached with\\ncurrent sample sizes',\n",
        "            ha='center', va='center', fontsize=12, transform=ax4.transAxes)\n",
        "    ax4.set_title('Sample Efficiency Analysis', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('few_shot_learning_curves.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nтЬЕ Learning curves saved to 'few_shot_learning_curves.png'\")\n"
      ],
      "metadata": {
        "id": "lsM3OvW5W4al"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 11: COMPREHENSIVE RESULTS TABLE\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 11: COMPREHENSIVE RESULTS TABLE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Create results DataFrame\n",
        "results_df = pd.DataFrame({\n",
        "    'Training Samples': ['Zero-shot (0)'] + [f'{n} samples' for n in few_shot_results['hindi']['sizes']],\n",
        "    'Accuracy': [zero_shot_results['hindi']] + few_shot_results['hindi']['accuracies'],\n",
        "    'F1-Score': [results['hindi']['f1']] + few_shot_results['hindi']['f1_scores'],\n",
        "    'Improvement (%)': [0.0] + [(acc - zero_shot_results['hindi'])*100\n",
        "                                 for acc in few_shot_results['hindi']['accuracies']]\n",
        "})\n",
        "\n",
        "print(\"\\nЁЯУК HINDI FEW-SHOT LEARNING RESULTS:\")\n",
        "print(\"=\"*80)\n",
        "print(results_df.to_string(index=False))\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Statistical Analysis\n",
        "print(\"\\nЁЯУИ KEY INSIGHTS:\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "max_improvement_idx = np.argmax(improvements)\n",
        "max_improvement = improvements[max_improvement_idx]\n",
        "samples_for_max = few_shot_results['hindi']['sizes'][max_improvement_idx]\n",
        "\n",
        "print(f\"1. Zero-shot Accuracy: {zero_shot_results['hindi']:.4f}\")\n",
        "print(f\"2. Best Few-shot Accuracy: {max(few_shot_results['hindi']['accuracies']):.4f} \"\n",
        "      f\"({samples_for_max} samples)\")\n",
        "print(f\"3. Maximum Improvement: +{max_improvement:.2f}% ({samples_for_max} samples)\")\n",
        "print(f\"4. Average Improvement: +{np.mean(improvements):.2f}%\")\n",
        "\n",
        "# Calculate efficiency metrics\n",
        "if len(few_shot_results['hindi']['sizes']) > 1:\n",
        "    # Marginal gain per additional sample\n",
        "    marginal_gains = []\n",
        "    for i in range(1, len(few_shot_results['hindi']['sizes'])):\n",
        "        prev_acc = few_shot_results['hindi']['accuracies'][i-1]\n",
        "        curr_acc = few_shot_results['hindi']['accuracies'][i]\n",
        "        prev_size = few_shot_results['hindi']['sizes'][i-1]\n",
        "        curr_size = few_shot_results['hindi']['sizes'][i]\n",
        "\n",
        "        marginal_gain = (curr_acc - prev_acc) / (curr_size - prev_size)\n",
        "        marginal_gains.append(marginal_gain)\n",
        "\n",
        "    print(f\"5. Diminishing Returns: {'Yes' if marginal_gains[-1] < marginal_gains[0] else 'No'}\")\n",
        "    print(f\"   - Initial marginal gain: {marginal_gains[0]*100:.3f}% per sample\")\n",
        "    print(f\"   - Final marginal gain: {marginal_gains[-1]*100:.3f}% per sample\")\n",
        "\n",
        "print(\"\\nЁЯТб RECOMMENDATIONS:\")\n",
        "print(\"-\" * 80)\n",
        "if max_improvement > 10:\n",
        "    print(\"тЬЕ Few-shot learning provides SIGNIFICANT improvement\")\n",
        "    print(f\"тЬЕ With just {samples_for_max} samples, accuracy improves by {max_improvement:.1f}%\")\n",
        "else:\n",
        "    print(\"тЪая╕П  Few-shot learning provides MODERATE improvement\")\n",
        "    print(\"тЪая╕П  Consider using more training samples or better data quality\")\n",
        "\n",
        "# Save comprehensive results\n",
        "comprehensive_results = {\n",
        "    'model': MODEL_NAME,\n",
        "    'zero_shot': zero_shot_results,\n",
        "    'few_shot': {\n",
        "        'hindi': {\n",
        "            'sample_sizes': few_shot_results['hindi']['sizes'],\n",
        "            'accuracies': few_shot_results['hindi']['accuracies'],\n",
        "            'f1_scores': few_shot_results['hindi']['f1_scores'],\n",
        "            'improvements': improvements\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "with open('comprehensive_results.json', 'w') as f:\n",
        "    json.dump(comprehensive_results, f, indent=2)\n",
        "\n",
        "print(\"\\nЁЯТ╛ Comprehensive results saved to 'comprehensive_results.json'\")"
      ],
      "metadata": {
        "id": "tpcCstgYXLO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U transformers accelerate datasets\n"
      ],
      "metadata": {
        "id": "vaFQ7SeearSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 12: MODEL COMPARISON (Research Question 2)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 12: MODEL COMPARISON ACROSS ARCHITECTURES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\"\"\n",
        "ЁЯОп EXPERIMENT: Compare Zero-Shot Transfer Across Different Models\n",
        "   Research Question: Which multilingual model transfers best?\n",
        "\n",
        "   Models to compare:\n",
        "   1. XLM-RoBERTa (xlm-roberta-base)\n",
        "   2. mBERT (bert-base-multilingual-cased)\n",
        "   3. IndicBERT (ai4bharat/indic-bert)\n",
        "\"\"\")\n",
        "\n",
        "# Store results for all models\n",
        "all_models_results = {}\n",
        "\n",
        "def train_and_evaluate_model(model_name, train_texts, train_labels, val_texts, val_labels,\n",
        "                             test_dfs, model_description):\n",
        "    \"\"\"Train a model and evaluate on all Indic languages\"\"\"\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Training {model_description}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    # Load tokenizer\n",
        "    print(\"ЁЯУж Loading tokenizer...\")\n",
        "    if 'indic-bert' in model_name:\n",
        "        tok = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "    else:\n",
        "        tok = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # Tokenize\n",
        "    print(\"ЁЯФд Tokenizing data...\")\n",
        "    train_enc = tok(train_texts, truncation=True, padding=True, max_length=256, return_tensors='pt')\n",
        "    val_enc = tok(val_texts, truncation=True, padding=True, max_length=256, return_tensors='pt')\n",
        "\n",
        "    train_ds = SentimentDataset(train_enc, train_labels)\n",
        "    val_ds = SentimentDataset(val_enc, val_labels)\n",
        "\n",
        "    # Load model\n",
        "    print(f\"ЁЯФз Loading {model_name}...\")\n",
        "    mdl = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
        "\n",
        "    # Training\n",
        "    args = TrainingArguments(\n",
        "        output_dir=f'./results_{model_name.replace(\"/\", \"_\")}',\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=32,\n",
        "        warmup_steps=500,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir='./logs',\n",
        "        logging_steps=100,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"eval_loss\",\n",
        "        greater_is_better=False,\n",
        "        report_to=\"none\",\n",
        "        seed=42\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=mdl,\n",
        "        args=args,\n",
        "        train_dataset=train_ds,\n",
        "        eval_dataset=val_ds,\n",
        "        compute_metrics=compute_metrics,\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
        "    )\n",
        "\n",
        "    print(\"ЁЯЪА Training...\")\n",
        "    trainer.train()\n",
        "\n",
        "    # Evaluate on English\n",
        "    eval_res = trainer.evaluate()\n",
        "    print(f\"тЬЕ English Val Accuracy: {eval_res['eval_accuracy']:.4f}\")\n",
        "\n",
        "    # Evaluate on Indic languages\n",
        "    results = {}\n",
        "    for lang, df in test_dfs.items():\n",
        "        print(f\"\\nЁЯФН Evaluating on {lang.capitalize()}...\")\n",
        "        res = evaluate_on_indic(mdl, tok, df, lang.capitalize())\n",
        "        results[lang] = {\n",
        "            'accuracy': res['accuracy'],\n",
        "            'f1': res['f1']\n",
        "        }\n",
        "\n",
        "    # Cleanup\n",
        "    del mdl, trainer\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return {\n",
        "        'model': model_name,\n",
        "        'description': model_description,\n",
        "        'english_val': {'accuracy': eval_res['eval_accuracy'], 'f1': eval_res['eval_f1']},\n",
        "        'indic_results': results\n",
        "    }\n",
        "\n",
        "# Prepare test dataframes\n",
        "test_dataframes = {\n",
        "    'hindi': hindi_test,\n",
        "    'telugu': telugu_test,\n",
        "    'tamil': tamil_test\n",
        "}\n",
        "\n",
        "# Run comparison (comment out models you don't want to test)\n",
        "print(\"\\nтЪая╕П  NOTE: This will train 3 models. It may take 30-45 minutes.\")\n",
        "print(\"To save time, you can comment out models in the list below.\\n\")\n",
        "\n",
        "models_to_compare = [\n",
        "    ('xlm-roberta-base', 'XLM-RoBERTa'),\n",
        "    ('bert-base-multilingual-cased', 'mBERT'),\n",
        "    # ('ai4bharat/indic-bert', 'IndicBERT'),  # Uncomment to include\n",
        "]\n",
        "\n",
        "for model_name, model_desc in models_to_compare:\n",
        "    result = train_and_evaluate_model(\n",
        "        model_name,\n",
        "        train_texts[:3000],  # Use subset for faster comparison\n",
        "        train_labels[:3000],\n",
        "        val_texts[:500],\n",
        "        val_labels[:500],\n",
        "        test_dataframes,\n",
        "        model_desc\n",
        "    )\n",
        "    all_models_results[model_name] = result\n",
        "    print(f\"\\nтЬЕ Completed {model_desc}\")\n",
        "\n",
        "# Create comparison table\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL COMPARISON RESULTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "comparison_data = []\n",
        "for model_key, res in all_models_results.items():\n",
        "    row = {\n",
        "        'Model': res['description'],\n",
        "        'Eng_Acc': res['english_val']['accuracy'],\n",
        "        'Hindi_Acc': res['indic_results']['hindi']['accuracy'],\n",
        "        'Hindi_F1': res['indic_results']['hindi']['f1'],\n",
        "        'Telugu_Acc': res['indic_results']['telugu']['accuracy'],\n",
        "        'Telugu_F1': res['indic_results']['telugu']['f1'],\n",
        "        'Tamil_Acc': res['indic_results']['tamil']['accuracy'],\n",
        "        'Tamil_F1': res['indic_results']['tamil']['f1'],\n",
        "    }\n",
        "    comparison_data.append(row)\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "print(\"\\nЁЯУК Zero-Shot Cross-Lingual Transfer Comparison:\")\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Accuracy comparison\n",
        "ax1 = axes[0]\n",
        "x = np.arange(len(comparison_data))\n",
        "width = 0.25\n",
        "\n",
        "hindi_accs = [row['Hindi_Acc'] for row in comparison_data]\n",
        "telugu_accs = [row['Telugu_Acc'] for row in comparison_data]\n",
        "tamil_accs = [row['Tamil_Acc'] for row in comparison_data]\n",
        "\n",
        "ax1.bar(x - width, hindi_accs, width, label='Hindi', color='#FF6B6B')\n",
        "ax1.bar(x, telugu_accs, width, label='Telugu', color='#4ECDC4')\n",
        "ax1.bar(x + width, tamil_accs, width, label='Tamil', color='#45B7D1')\n",
        "\n",
        "ax1.set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
        "ax1.set_title('Model Comparison: Zero-Shot Accuracy', fontsize=14, fontweight='bold')\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels([row['Model'] for row in comparison_data], rotation=15, ha='right')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3, axis='y')\n",
        "ax1.set_ylim([0, 1])\n",
        "\n",
        "# F1 comparison\n",
        "ax2 = axes[1]\n",
        "hindi_f1s = [row['Hindi_F1'] for row in comparison_data]\n",
        "telugu_f1s = [row['Telugu_F1'] for row in comparison_data]\n",
        "tamil_f1s = [row['Tamil_F1'] for row in comparison_data]\n",
        "\n",
        "ax2.bar(x - width, hindi_f1s, width, label='Hindi', color='#FF6B6B')\n",
        "ax2.bar(x, telugu_f1s, width, label='Telugu', color='#4ECDC4')\n",
        "ax2.bar(x + width, tamil_f1s, width, label='Tamil', color='#45B7D1')\n",
        "\n",
        "ax2.set_ylabel('F1-Score', fontsize=12, fontweight='bold')\n",
        "ax2.set_title('Model Comparison: Zero-Shot F1-Score', fontsize=14, fontweight='bold')\n",
        "ax2.set_xticks(x)\n",
        "ax2.set_xticklabels([row['Model'] for row in comparison_data], rotation=15, ha='right')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3, axis='y')\n",
        "ax2.set_ylim([0, 1])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nтЬЕ Model comparison saved to 'model_comparison.png'\")\n",
        "\n",
        "# Key insights\n",
        "print(\"\\nЁЯТб KEY INSIGHTS:\")\n",
        "print(\"-\" * 80)\n",
        "best_hindi = max(comparison_data, key=lambda x: x['Hindi_Acc'])\n",
        "best_telugu = max(comparison_data, key=lambda x: x['Telugu_Acc'])\n",
        "best_tamil = max(comparison_data, key=lambda x: x['Tamil_Acc'])\n",
        "\n",
        "print(f\"Best for Hindi: {best_hindi['Model']} ({best_hindi['Hindi_Acc']:.4f})\")\n",
        "print(f\"Best for Telugu: {best_telugu['Model']} ({best_telugu['Telugu_Acc']:.4f})\")\n",
        "print(f\"Best for Tamil: {best_tamil['Model']} ({best_tamil['Tamil_Acc']:.4f})\")\n",
        "\n",
        "# Save model comparison\n",
        "with open('model_comparison_results.json', 'w') as f:\n",
        "    json.dump(all_models_results, f, indent=2, default=float)\n",
        "\n",
        "print(\"\\nЁЯТ╛ Model comparison saved to 'model_comparison_results.json'\")\n"
      ],
      "metadata": {
        "id": "-JEQ5_PUXR86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 13: LAYER-WISE TRANSFER ANALYSIS (Research Question 4)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 13: LAYER-WISE TRANSFER ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\"\"\n",
        "ЁЯОп EXPERIMENT: Which transformer layers encode language-agnostic knowledge?\n",
        "   Research Question: Which layers encode language-agnostic sentiment knowledge?\n",
        "\n",
        "   Method: Train linear probes on each layer's CLS embeddings\n",
        "\n",
        "   Expected Pattern:\n",
        "   - Lower layers (0-3): Language-specific features\n",
        "   - Middle layers (4-8): Cross-lingual semantic features\n",
        "   - Upper layers (9-11): Task-specific features\n",
        "\"\"\")\n",
        "\n",
        "def extract_layer_embeddings(model, tokenizer, texts, layer_idx):\n",
        "    \"\"\"Extract CLS embeddings from a specific layer\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize\n",
        "    encodings = tokenizer(texts, truncation=True, padding=True,\n",
        "                         max_length=256, return_tensors='pt').to(device)\n",
        "\n",
        "    # Get hidden states\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**encodings, output_hidden_states=True)\n",
        "        # hidden_states: (num_layers, batch_size, seq_len, hidden_dim)\n",
        "        layer_output = outputs.hidden_states[layer_idx]\n",
        "        # Extract CLS token (first token)\n",
        "        cls_embeddings = layer_output[:, 0, :].cpu().numpy()\n",
        "\n",
        "    return cls_embeddings\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def probe_layer(model, tokenizer, train_texts, train_labels,\n",
        "                test_texts, test_labels, layer_idx, layer_name):\n",
        "    \"\"\"Train and evaluate a probe on a specific layer\"\"\"\n",
        "\n",
        "    # Extract embeddings\n",
        "    train_emb = extract_layer_embeddings(model, tokenizer, train_texts, layer_idx)\n",
        "    test_emb = extract_layer_embeddings(model, tokenizer, test_texts, layer_idx)\n",
        "\n",
        "    # Normalize\n",
        "    scaler = StandardScaler()\n",
        "    train_emb = scaler.fit_transform(train_emb)\n",
        "    test_emb = scaler.transform(test_emb)\n",
        "\n",
        "    # Train probe\n",
        "    probe = LogisticRegression(max_iter=1000, random_state=42)\n",
        "    probe.fit(train_emb, train_labels)\n",
        "\n",
        "    # Evaluate\n",
        "    acc = probe.score(test_emb, test_labels)\n",
        "\n",
        "    return acc\n",
        "\n",
        "print(\"\\nЁЯФм Running layer-wise probing analysis...\")\n",
        "print(\"Note: Using the first trained model from previous experiments\\n\")\n",
        "\n",
        "# Use the first model that was trained\n",
        "probe_model_name = list(all_models_results.keys())[0]\n",
        "print(f\"Using model: {all_models_results[probe_model_name]['description']}\")\n",
        "\n",
        "# Load the model\n",
        "probe_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    probe_model_name, num_labels=2\n",
        ").to(device)\n",
        "\n",
        "# Load tokenizer\n",
        "if 'indic-bert' in probe_model_name:\n",
        "    probe_tokenizer = AutoTokenizer.from_pretrained(probe_model_name, trust_remote_code=True)\n",
        "else:\n",
        "    probe_tokenizer = AutoTokenizer.from_pretrained(probe_model_name)\n",
        "\n",
        "# Sample data for probing (use small subset for speed)\n",
        "probe_train_texts = train_texts[:500]\n",
        "probe_train_labels = train_labels[:500]\n",
        "\n",
        "# Probe each language\n",
        "layer_results = {\n",
        "    'hindi': [],\n",
        "    'telugu': [],\n",
        "    'tamil': []\n",
        "}\n",
        "\n",
        "# Number of layers (12 for BERT-based models)\n",
        "num_layers = 13  # 0 = embeddings, 1-12 = transformer layers\n",
        "\n",
        "print(\"\\nProbing layers...\")\n",
        "for layer_idx in tqdm(range(num_layers)):\n",
        "    layer_name = f\"Layer {layer_idx}\" if layer_idx > 0 else \"Embeddings\"\n",
        "\n",
        "    # Probe Hindi\n",
        "    hindi_acc = probe_layer(\n",
        "        probe_model, probe_tokenizer,\n",
        "        probe_train_texts, probe_train_labels,\n",
        "        hindi_test['text'].head(50).tolist(), hindi_test['label'].head(50).tolist(),\n",
        "        layer_idx, layer_name\n",
        "    )\n",
        "    layer_results['hindi'].append(hindi_acc)\n",
        "\n",
        "    # Probe Telugu\n",
        "    telugu_acc = probe_layer(\n",
        "        probe_model, probe_tokenizer,\n",
        "        probe_train_texts, probe_train_labels,\n",
        "        telugu_test['text'].head(50).tolist(), telugu_test['label'].head(50).tolist(),\n",
        "        layer_idx, layer_name\n",
        "    )\n",
        "    layer_results['telugu'].append(telugu_acc)\n",
        "\n",
        "    # Probe Tamil\n",
        "    tamil_acc = probe_layer(\n",
        "        probe_model, probe_tokenizer,\n",
        "        probe_train_texts, probe_train_labels,\n",
        "        tamil_test['text'].head(50).tolist(), tamil_test['label'].head(50).tolist(),\n",
        "        layer_idx, layer_name\n",
        "    )\n",
        "    layer_results['tamil'].append(tamil_acc)\n",
        "\n",
        "# Visualization\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "layers = list(range(num_layers))\n",
        "ax.plot(layers, layer_results['hindi'], marker='o', linewidth=2, label='Hindi', color='#FF6B6B')\n",
        "ax.plot(layers, layer_results['telugu'], marker='s', linewidth=2, label='Telugu', color='#4ECDC4')\n",
        "ax.plot(layers, layer_results['tamil'], marker='^', linewidth=2, label='Tamil', color='#45B7D1')\n",
        "\n",
        "ax.set_xlabel('Layer Index', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('Probe Accuracy', fontsize=12, fontweight='bold')\n",
        "ax.set_title('Layer-Wise Transfer Analysis: Which Layers Encode Cross-Lingual Sentiment?',\n",
        "            fontsize=14, fontweight='bold')\n",
        "ax.legend(fontsize=11)\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.set_xticks(layers)\n",
        "ax.set_xticklabels(['Emb'] + [str(i) for i in range(1, num_layers)])\n",
        "\n",
        "# Highlight middle layers\n",
        "ax.axvspan(4, 8, alpha=0.1, color='green', label='Middle Layers\\n(Cross-lingual)')\n",
        "ax.set_ylim([0.4, 1.0])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('layer_wise_analysis.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nтЬЕ Layer-wise analysis saved to 'layer_wise_analysis.png'\")\n",
        "\n",
        "# Analysis\n",
        "print(\"\\nЁЯТб LAYER-WISE INSIGHTS:\")\n",
        "print(\"-\" * 80)\n",
        "for lang in ['hindi', 'telugu', 'tamil']:\n",
        "    best_layer = np.argmax(layer_results[lang])\n",
        "    best_acc = layer_results[lang][best_layer]\n",
        "    print(f\"{lang.capitalize()}:\")\n",
        "    print(f\"  Best layer: {best_layer} ({best_acc:.4f} accuracy)\")\n",
        "    print(f\"  Lower layers avg (0-3): {np.mean(layer_results[lang][:4]):.4f}\")\n",
        "    print(f\"  Middle layers avg (4-8): {np.mean(layer_results[lang][4:9]):.4f}\")\n",
        "    print(f\"  Upper layers avg (9-12): {np.mean(layer_results[lang][9:]):.4f}\")\n",
        "    print()\n",
        "\n",
        "print(\"\\nЁЯФм RESEARCH FINDING:\")\n",
        "if all(np.argmax(layer_results[lang]) >= 4 and np.argmax(layer_results[lang]) <= 8\n",
        "       for lang in ['hindi', 'telugu', 'tamil']):\n",
        "    print(\"тЬЕ Middle layers (4-8) consistently show best cross-lingual transfer\")\n",
        "    print(\"   This confirms that semantic knowledge is language-agnostic\")\n",
        "else:\n",
        "    print(\"тЪая╕П  Transfer pattern varies across languages\")\n",
        "    print(\"   Language families may influence optimal layer depth\")\n",
        "\n",
        "# Cleanup\n",
        "del probe_model\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Save layer results\n",
        "with open('layer_wise_results.json', 'w') as f:\n",
        "    json.dump(layer_results, f, indent=2)\n",
        "\n",
        "print(\"\\nЁЯТ╛ Layer-wise results saved to 'layer_wise_results.json'\")\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"тЬЕ NOTEBOOK COMPLETE!\")\n",
        "print(\"=\"*80)\n"
      ],
      "metadata": {
        "id": "fyDU-ER1XTYy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM2yr0hbqTQ764Hn9DuTVQD",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}